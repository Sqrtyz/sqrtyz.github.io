<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Computer Organization Note (Part 4 of 4) | Yuzhuo Tian </title> <meta name="author" content="Yuzhuo Tian"> <meta name="description" content="Contains content about memory and hierarchy storage."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?f566e11424fb8bc11d484dac03de3f7f"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sqrtyz.github.io/blog/2024/CO-NOTE-4/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yuzhuo</span> Tian </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Computer Organization Note (Part 4 of 4)</h1> <p class="post-meta"> Created on January 01, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/hardware"> <i class="fa-solid fa-hashtag fa-sm"></i> Hardware</a>   ·   <a href="/blog/category/coursenotes"> <i class="fa-solid fa-tag fa-sm"></i> CourseNotes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="chapter-5-memory-hierarchy">Chapter 5 Memory Hierarchy</h2> <h3 id="introduction">Introduction</h3> <h4 id="transistor-晶体管">Transistor 晶体管</h4> <p><img src="/assets/img/blog_post/co/cod-84.png" alt="cod-84" width="80%"></p> <p>类似于一个开关。栅极为 1 时两侧的 N 型半导体硅连通，input 连接到 output。</p> <h4 id="basic-memory-types">Basic Memory Types</h4> <ul> <li> <p>SRAM (Static Random Access Memory)</p> <p>六个晶体管。static 是指这种存储器只需要保持通电，里面的数据就可以永远保持。但是当断点之后，里面的数据仍然会丢失。</p> <p>SRAM 速度较快，但是成本更高，占用的空间更多。所以像诸如 CPU 的高速缓存，才会采用 SRAM。</p> <p><img src="/assets/img/blog_post/co/cod-85.png" alt="cod-85" width="40%"></p> </li> <li> <p>DRAM (Dynamic Random Access Memory)</p> <p>一个电容加一个晶体管。由于 DRAM 使用电容存储，所以必须隔一段时间 refresh 一次，否则因为不断的微小漏电可能导致数据丢失。</p> <p>DRAM 速度较慢，但是成本更低。所以可以用于作为 main memory。</p> <p><img src="/assets/img/blog_post/co/cod-86.png" alt="cod-86" width="40%"></p> </li> <li> <p>Disk Storage</p> <p><img src="/assets/img/blog_post/co/cod-98.png" alt="cod-98" width="80%"></p> <ul> <li> <p>Structure</p> <ul> <li> <p><strong>Platter</strong> 盘子。一个磁盘一般有 4-16 个盘子。</p> </li> <li> <p><strong>Track</strong> 轨道。对应图上不同半径的环。</p> </li> <li> <p><strong>Sector</strong> 扇区。每个 track 又被分割为若干 sectors。</p> </li> <li> <p><strong>Cylinder</strong> 所有 platter 同一半径的 track 集合。</p> </li> <li> <p><strong>Read-write head</strong> 读写头。</p> </li> </ul> </li> <li> <p>Average Read Time of Disks</p> <p>基本方法为考虑四部分：</p> <ol> <li>寻道时间 Seek time</li> <li>旋转等待时间 Rotational latency</li> <li>数据传输时间 Transfer time</li> <li>控制用时 Control time</li> </ol> <p>【例题】Given 512B sector, 15,000 rpm, 4ms average seek time, 100MB/s transfer rate, 0.2ms controller overhead. Calculate average read time.</p> <ol> <li> <strong>4ms</strong> seek time</li> <li>$\frac{1}{2} \times \frac{15000}{60}$ = <strong>2ms</strong> rotational latency（1/2 表示期望）</li> <li>$\frac{512B}{100MB/s}$ = <strong>0.005ms</strong> transfer time</li> <li> <strong>0.2ms</strong> controller delay</li> </ol> <p>Total: <strong>6.2ms</strong></p> </li> </ul> </li> <li> <p>Flash Storage</p> <p>中文为闪存。比硬盘快了 100~1000 倍。Flash 又分为 NOR flash 和 NAND flash。</p> <p>我们熟知的 SSD（固态硬盘）一般就使用闪存存储数据。</p> </li> </ul> <h4 id="memory-hierarchy">Memory Hierarchy</h4> <p>Hierarchies bases on memories of different speeds and size.</p> <p>金字塔越往上，memory 更 expensive / small / fast。</p> <p><img src="/assets/img/blog_post/co/cod-87.png" alt="cod-87" width="90%"></p> <h3 id="cache-basics">Cache Basics</h3> <h4 id="cache-intro---direct-mapped-cache">Cache Intro - Direct Mapped Cache</h4> <ul> <li> <p>基本思路</p> <p><img src="/assets/img/blog_post/co/cod-88.png" alt="cod-88" width="80%"></p> <p><strong>(Cache Block Address)</strong> = <strong>(Memory Block Address)</strong> mod (Number of blocks in the cache)</p> <p>注意，一个 block 中不是一个 byte。RISC-V 中我们默认一个 block 的 <strong>最小大小</strong> 为 word (4 byte)。</p> <p>为了得知一个 cache block 中究竟存的是哪一个 memory block 的数据（例如上图中，一个灰色块可以对应 8 个灰色块），cache block 还会存下「其所存储的数据」对应的 memory block 地址的「高若干位」。具体地，一个 DMC 的结构如下所示：</p> </li> <li> <p>DMC 的结构</p> <p><img src="/assets/img/blog_post/co/cod-89.png" alt="cod-89" width="60%"></p> <ol> <li> <p>V 表示 Valid bit，表示是否启用。启用为 1，反之为 0。</p> </li> <li> <p>Tag 即为刚刚所说的，「其所存储的数据」对应的 memory block 地址的「高若干位」。例如，对于上图例 Memory 中的最左边的灰色块，它的数据在 Cache 中存储在 001 的 Data 段，Tag 段对应为 00。</p> </li> <li> <p>Data 存储的就是数据。如果一个 block 的大小为 1 word，那么 Data 段的长度就是 32 位。</p> </li> </ol> </li> <li> <p>DMC 的寻址</p> <p>考虑一个情景，CPU 希望从内存中抓一个地址（记得是按 Byte 寻址哦）。这个 64 位的地址将会被分为 TAG / Index / Byte offset。其中 {Tag, Index} 联称 Memory Block Address。</p> <p>注意，当一个 block 包含 1 word 时，byte offset 占 2 位；但有时候一个 block 也可以占更多的字（比如 4 word），此时 byte offset 会占更多位（比如 4 位）。</p> <p><img src="/assets/img/blog_post/co/cod-90.png" alt="cod-90" width="80%"></p> </li> </ul> <h4 id="cache-topic-1---block-placement">Cache Topic 1 - Block Placement</h4> <p><img src="/assets/img/blog_post/co/cod-91.png" alt="cod-91" width="80%"></p> <p>除了 Direct Mapped，还有两种常见的映射策略：</p> <ul> <li> <p>Fully Associative</p> <p>内存中的 block 可以对到 cache 中的任何 block。好处是提高了 cache 的利用率，但是寻址会很麻烦。</p> </li> <li> <p>Set Associative</p> <p>介于 Direct Mapped 和 Fully Associative 之间。内存中的 block 可以对到 cache 中 <strong>某个 set</strong> 的任何 block。set 的选取和 block 一致，即 <strong>(Cache Set Address)</strong> = <strong>(Memory Block Address)</strong> mod <strong>(Number of sets in the cache)</strong></p> <p>对于 Set Associative，一个 set 如果包含 $k$ blocks，那么这个 cache 又被称为 $k$-way set associative。DM 和 FA 可以视作特殊的 SA。比如 DM 就是 $1$-way SA，FA 就是 $m$-way SA（其中 $m$ 表示 block 数目）。</p> </li> </ul> <h4 id="cache-topic-2---block-identification">Cache Topic 2 - Block Identification</h4> <p>基本思路就是按照 Direct Mapped 那样。</p> <p><img src="/assets/img/blog_post/co/cod-92.png" alt="cod-92" width="50%"></p> <p>Direct Mapped 的具体寻址方式之前已经介绍。</p> <p>对于 Fully Associative，寻址方式如下：</p> <p><img src="/assets/img/blog_post/co/cod-93.png" alt="cod-93" width="70%"></p> <p>对于 Set Associative，寻址方式如下：</p> <p><img src="/assets/img/blog_post/co/cod-94.png" alt="cod-94" width="80%"></p> <h4 id="cache-topic-3---block-replacement">Cache Topic 3 - Block Replacement</h4> <p>如果一个 cache 满了，再从内存往里面放东西的话（例如 read miss），我们需要对其中的数据进行替代。</p> <ul> <li> <p>对于 Direct Mapped，没有这方面的担心。因为映射是多对一的，每次要么换要么不换。</p> </li> <li> <p>对于 Fully Associative 和 Set Associative，则需要考虑换掉哪个 block。一般有如下的策略：</p> <ol> <li> <p>Random replacement 字面意思，随机替换。</p> </li> <li> <p>Least-recently used (LRU) 最长时间没用的替换掉。</p> </li> <li> <p>First in, first out (FIFO) 队列思想，最早引入 cache 的替换掉。注意这和 LRU 不一样。</p> </li> </ol> </li> </ul> <h4 id="cache-topic-4---read-and-write-strategy">Cache Topic 4 - Read and Write Strategy</h4> <ul> <li> <p>Read Strategy</p> <ul> <li> <p>如果 Read hits，无事发生，这很好。</p> </li> <li> <p>如果 Read misses，我们需要执行一系列措施。实际上，Read misses 分为 instruction cache miss 和 data cache miss。以 instruction cache miss 为例：</p> <ol> <li> <p>将目前的 PC 值告诉 Memory，意思是我接下来要读 PC 对应的指令了，但是我在 Cache 部分没找到，所以要去 Memory 部分找。</p> </li> <li> <p>Memory access 到对应的 32 位指令。</p> </li> <li> <p>将对应的指令写进 instruction cache。其中 data 段放入指令本身，tag 段放入高位地址（ALU 来计算高位），valid bit 要设为 1。</p> </li> <li> <p>重启 PC 指令的 fetching，这次我们可以在 instruction cache 读取到它。</p> </li> </ol> </li> </ul> </li> <li> <p>Write Strategy</p> <ul> <li> <p>Write Hit Strategy</p> <ol> <li> <p>Write-back 只向缓存中写入数据，回头找个合适的时机写入内存。好处是很快，但是会导致数据的 inconsistent。</p> <p>此外，Write-back 不能直接丢弃 cached data（例如由 read miss 引起的替换），这是因为其中的值可能并未被同步到 memory。故而 cache 的 control bit 需要使用两位：valid bit【依然和原来一样，表示是否数据有效】和 dirty bit【表示是否同步到 memory 中，未同步则为 0】。如果替换时发现被替换者 dirty bit 为 0，则需要先将被替换者写入内存。</p> </li> <li> <p>Write-through 则同时向缓存和内存写入数据。好处是可以保证数据的 consistency，但是速度会较慢。</p> <p>对于 Write-through，我们可以放心地丢弃 cached data，因为其中的值和 memory 始终保持同步。此类 cache 的 control bit 只有 valid bit 一位。</p> </li> </ol> </li> <li> <p>Write Stall and Write Buffers</p> <p><strong>Write stall</strong> 指的是在 Write-through 中，CPU 必须等待 write to memory 完成所导致的 bubble。</p> <p><strong>Write buffer</strong> 如下图所示。对于 Write-through 的策略，由于理论上我们需要时刻把 cache 和 memory 保持同步，而这会耗费大量的时间。所以考虑 cache 同步到 memory 时，我们先把它写到一个 buffer 里面（这很快），写完后 CPU 继续干自己的活，buffer 则慢慢地把数据同步到 memory 中。</p> <p><img src="/assets/img/blog_post/co/cod-95.png" alt="cod-95" width="80%"></p> </li> <li> <p>Write Miss Strategy</p> <ol> <li> <p>若 Write Hit Strategy 采取 Write-back，则 miss 时一般采取 Write allocate 策略（理论上也可以采取 Write around 策略）。</p> <p>Write allocate：<strong>如果 Write misses，先把对应的内存数据读到缓存中</strong>，转换为 Write hits 的情况再后续处理。</p> <p>注意，由于一个 block 可能包含若干个 word，所以我们也有必要这样做。考虑情景：本来一个 block 对应的 4 words 分别为 [A B C D]，考虑只写其中的一个 word。如果 write miss 发生，cache 中的对应 block 变为 [X E X X]（其中 [X X X X] 是对应 cache entry 的原始值），后面写到内存中，变为 [X E X X]，但实际上应该是 [A E C D]。</p> </li> <li> <p>若 Write Hit Strategy 采取 Write-through，则 miss 时一般采取 Write around 策略（理论上也可以采取 Write allocate 策略）。</p> <p>Write around：<strong>如果 Write misses</strong>，直接绕过 cache 把对应的数据写入 memory。想来确实也可以这么干。</p> </li> </ol> </li> </ul> </li> </ul> <h4 id="cache-memory-data-transfer">Cache-Memory Data Transfer</h4> <p>之前提到，如果发生 read miss 或者 write miss，则需要在 cache 与 memory 之间进行数据传输。对于不同的 Cache-Memory 结构，其传输效率也不一。</p> <p><img src="/assets/img/blog_post/co/cod-96.png" alt="cod-96" width="80%"></p> <p>假设耗时（假设 Block size 为 4 words）：</p> <table> <thead> <tr> <th>Work</th> <th>CC Cost</th> </tr> </thead> <tbody> <tr> <td>send the address</td> <td>1</td> </tr> <tr> <td>DRAM access initiated</td> <td>15</td> </tr> <tr> <td>transfer a word of data</td> <td>1</td> </tr> <tr> <td>send the address</td> <td>1</td> </tr> </tbody> </table> <ol> <li> <p>One-word-wide memory organization</p> <p>最基本的架构。在以上假设下，读取一个 block 的耗时为 $1 + 4 \times (1+15)=65$ CC</p> </li> <li> <p>Wide memory organization</p> <p>更宽的总线和内存。在以上假设下，读取一个 block 的耗时为 $1 + (1+15)=17$ CC</p> <p>然而，随之而来的代价是硬件设计的要求提高（最直观的，占空间变多了）</p> </li> <li> <p>Interleaved memory organization</p> <p>内存分成几个 bank，这样一来内存部分可以同步进行 initialize。在以上假设下，读取一个 block 的耗时为 $1 + 4 \times 1 + 15=20$ CC</p> </li> </ol> <h3 id="measure-and-improve-cache-performance">Measure and Improve Cache Performance</h3> <h4 id="一些指标">一些指标</h4> <ul> <li> <p>Average Memory Assess Time (AMAT)</p> <p>= hit time + miss time</p> <p>= hit time + miss rate $\times$ miss penalty</p> </li> <li> <p>CPU Time (更新定义)</p> <p>= CPU execution clock cycles + Memory-stall clock cycles</p> </li> <li> <p>Memory-stall clock cycles</p> <p>= Number of instructions $\times$ miss rate $\times$ miss penalty</p> <p>(Also can be written as) = Read-stall cycles + Write-stall cycles</p> </li> <li> <p>Read-stall cycles</p> <p>= number of read instructions $\times$ read miss rate $\times$ read miss penalty</p> </li> <li> <p>Write-stall cycles <strong>(For Write-through strategy)</strong></p> <p>= number of write instructions $\times$ write miss rate $\times$ write miss penalty + write buffer stalls</p> </li> <li> <p>如果忽略 write buffer stalls，Read-stall cycles 和 Write-stall cycles 理论上可以合并。统一为</p> <p>Memory-stall clock cycles ＝ Memory access instructions $\times$ Miss rate $\times$ Miss penalty</p> </li> </ul> <h4 id="example-quiz">Example Quiz</h4> <p><img src="/assets/img/blog_post/co/cod-97.png" alt="cod-97" width="60%"></p> <h4 id="miss-penalty-一图流">Miss Penalty 一图流</h4> <p><img src="/assets/img/blog_post/co/cod-99.png" alt="cod-99" width="80%"></p> <h3 id="l1-and-l2-cache-hierarchy">L1 and L2 Cache Hierarchy</h3> <p>回顾之前的金字塔图，我们可以将 Cache 分为 L1 Cache 和 L2 Cache（两者均采用 SRAM，唯一的区别在于大小不同，L1 Cache 略快于 L2 Cache）。</p> <p>如果 L1 Cache miss 了，就去 L2 Cache 读取；如果再 miss，就去 DRAM。</p> <p>考虑这样一道计算题：</p> <ul> <li> <p>CPI of 1.0 on a 5GHz machine</p> </li> <li> <p>Initial: 2% miss rate, 100ns DRAM access</p> </li> <li> <p>Adding L2 Cache: 5ns access time, decreases miss rate to 0.5%</p> </li> </ul> <h3 id="virtual-memory">Virtual Memory</h3> <ul> <li> <p>Basic Concepts</p> <p>和之前印象中的 virtual memory 有点区别。似乎不是拿 disk 当 memory 用，据说 virtual memory 有很多所指。</p> <p>不管这些。我们接下来讨论的 virtual memory 实际上就是一个介于 CPU 和「实际地址」之间的「中介」。在实际过程中，CPU 只抛出虚拟地址，根据虚拟地址得到实际内存地址再去访问 cache memory (SRAM) / main memory (DRAM) / disk。</p> <p><img src="/assets/img/blog_post/co/cod-100.png" alt="cod-100" width="80%"></p> </li> <li> <p>Fetching Physical Address Method I: Page Table</p> <p>一般来说 page table 位于 main memory 中，就像一般的内存那样存储了从 virtual address 到 physical address 的映射。</p> <p>如果某个数据不在 physical memory 中，则 virtual address 映射到的会是 disk address。此时称发生了 page fault，同时这会导致极大的 miss penalty。</p> <p><img src="/assets/img/blog_post/co/cod-101.png" alt="cod-101" width="70%"></p> <p>下图展示了具体如何利用 page table 将 virtual address 映射到 physical address 的方法。前半部分拿进去查表，后半部分 offset 保持不变。当然，这只是 page table hit 的情况。</p> <p><img src="/assets/img/blog_post/co/cod-102.png" alt="cod-102" width="80%"></p> </li> <li> <p>Fetching Physical Address Method II: TLB</p> <p>为了加速从 virtual address 找 physical address 这一过程，我们可以添加一个 TLB (Translation-lookaside Buffer) 模块。</p> <p>TLB 可以视作一个「关于 page table 的 fully-associative cache」。注意 TLB 只存储映射到 physical memory 的情形。</p> <p><img src="/assets/img/blog_post/co/cod-103.png" alt="cod-103" width="80%"></p> </li> <li> <p>Whole Structure</p> <p>考虑 TLB 后，memory data 的获取流程如下（图中未显示 page table 的情形，不过应该比较好脑补）。</p> <p><img src="/assets/img/blog_post/co/cod-104.png" alt="cod-104" width="80%"></p> <p><img src="/assets/img/blog_post/co/cod-105.png" alt="cod-105" width="80%"></p> </li> <li> <p>理解检测</p> <p><img src="/assets/img/blog_post/co/cod-106.png" alt="cod-106" width="80%"></p> </li> </ul> <h2 id="chapter-6-storage-networks-and-other-peripherals">Chapter 6 Storage, Networks and Other Peripherals</h2> <h3 id="introduction-1">Introduction</h3> <ul> <li> <p>Typical I/O Devices</p> <p><img src="/assets/img/blog_post/co/cod-107.png" alt="cod-107" width="80%"></p> </li> <li> <p>Three Characters of I/O</p> <ol> <li> <p>Behavior</p> <p>行为。例如是做 input，还是 output，还是 storage。</p> </li> <li> <p>Partner</p> <p>对象。交互对象，例如 mouse 的 partner 是人，network 的 partner 是机器。</p> </li> <li> <p>Data Rate</p> <p>I/O device 和 main memory/processor 之间的数据传输速度峰值。</p> </li> </ol> </li> <li> <p>I/O Performance Measurement: <strong>Throughput</strong> and <strong>Response Time</strong></p> </li> </ul> <h3 id="disk-storage-and-dependability">Disk Storage and Dependability</h3> <ul> <li> <p>Availability Measurement</p> <ol> <li> <p>MTTF (Mean Time to Failure)</p> <p>平均故障时间。即一个部件期望的无故障运行时长。</p> </li> <li> <p>MTTR (Mean Time to Repair)</p> <p>平均修复时间。即一个部件发生故障后期望的修复所需时长。</p> </li> <li>MTBF (Mean Time Between Failures) = MTTF + MTTR</li> <li>Availability = MTTF / (MTTF + MTTR)</li> </ol> </li> <li> <p>Reliability Measurement</p> <p>考虑一个情景：我们能否使用非常多的小 disk 来组成大 disk，进而缩短 disk 和 CPU/Memory 之间的速度差距？</p> <p><img src="/assets/img/blog_post/co/cod-108.png" alt="cod-108" width="60%"></p> <p>答案是否定的。因为 (MTTF of N disks) = (MTTF of 1 Disk) / N。如此设计会让 disk 的可靠性大幅下降。</p> <p>实际上 reliability 的衡量建立于 availability 的衡量之上：</p> <ol> <li> <p>AFR (annual failure rate) = percentage of devices to fail per year</p> <p>= (365 $\times$ 24) hours / MTTF in hours</p> </li> <li> <p>“nines of availability” per year (中间是 availability，右边是 meaning)</p> <p><img src="/assets/img/blog_post/co/cod-109.png" alt="cod-109" width="60%"></p> </li> </ol> </li> <li> <p>Magnetic Disk</p> <p>详见 Chapter 5: Introduction: Basic Memory Type 部分。</p> </li> <li> <p>Flash Storage</p> <p>详见 Chapter 5: Introduction: Basic Memory Type 部分。</p> </li> <li> <p>RAID</p> <p><img src="/assets/img/blog_post/co/cod-110.png" alt="cod-110" width="60%"></p> <p>DBS 部分已有介绍。这边只作简要补充。</p> <ul> <li> <p>RAID 0: No Redundancy (Skipped)</p> </li> <li> <p>RAID 1: Disk Mirroring/Shadowing (Skipped)</p> </li> <li> <p>RAID 2: Error Correction Code (Skipped, <em>UNUSED</em> now)</p> </li> <li> <p>RAID 3: Bit-Interleaved Parity Disk</p> <p><img src="/assets/img/blog_post/co/cod-113.png" alt="cod-113" width="50%"></p> <p>其中 0, 1, 2, …, 23 是数据的实际顺序。但是每个方框只包含了一个 bit。</p> </li> <li> <p>RAID 4: Block-Interleaved Parity Disk</p> <p><img src="/assets/img/blog_post/co/cod-111.png" alt="cod-111" width="70%"></p> <p>其中 0, 1, 2, …, 23 是数据的实际顺序。每个方框只包含了一个 block。</p> <p>【写入分析 1】一次 Logical Write 将会涉及到 2 次 Physical Read 和 2 次 Physical Write（如上右图所示）。</p> <p>【写入分析 2】对比 RAID 3 和 RAID 4 的 small writes（绿色部分示意，假设 8bit）。RAID 3 需要进行 4 次串行的「Main Disk + Parity Disk」的物理读取和写入（无法并行是因为 Parity Disk 在 4 次中都需要被访问）；而 RAID 4 由于 8bit 都位于 block 0 中，所以只需要一次「Main Disk + Parity Disk」的物理读取和写入。</p> </li> <li> <p>RAID 5: Block-Interleaved Distributed Parity Disk</p> <p><img src="/assets/img/blog_post/co/cod-112.png" alt="cod-112" width="80%"></p> <p>其中 0, 1, 2, …, 23 是数据的实际顺序。</p> <p>【写入分析 3】这种分布式设计允许更为激进的并行写入。例如：我要 logical write D0 和 D5。对于 RAID 5，这 $2 \times 2$ 个 disks <strong>互不干扰</strong>，可以同时进行；但是对于 RAID 4，$P$ 校验位使用同一个 disk，logical write 必须分两波次进行。</p> </li> <li> <p>RAID 6: P + Q Redundancy</p> <p>在 RAID 5 的基础上，使用两个 Redundancy Disk。</p> </li> <li> <p>Comparison</p> <ol> <li>一般而言，RAID 3 比 RAID 4 更擅长长序列读取，RAID 4 比 RAID 3 更擅长小范围读取；</li> <li>RAID 3 在 small writes 上具有最低的 throughput；RAID 3. 4. 5 在 large writes 上具有几乎一致的 throughput。</li> </ol> </li> </ul> </li> </ul> <h3 id="buses">Buses</h3> <h4 id="buses-basics">Buses Basics</h4> <ul> <li> <p>Buses</p> <p>总线是多条线的组合，用于进行各模块之间的数据传输。</p> <p>分为 control lines 和 data lines，其中 data lines 可以传输地址和具体数据。</p> </li> <li> <p>Bus Transactions</p> <ol> <li> <p>Output 流程（CPU 指示内存向 devices 中写出数据）</p> <p><img src="/assets/img/blog_post/co/cod-114.png" alt="cod-114" width="80%"></p> </li> <li> <p>Input 流程（CPU 指示内存从 devices 中读取数据）</p> <p><img src="/assets/img/blog_post/co/cod-115.png" alt="cod-115" width="70%"></p> </li> </ol> </li> </ul> <h4 id="asynchronous-data-fetching-handshaking-protocol">Asynchronous Data Fetching: Handshaking Protocol</h4> <p>考虑一个情形：某个 I/O Device 希望从 memory 中读取数据。在「同步」和「异步」的情况下，读取方法有所差异。接下来介绍异步读取的「握手协议」：</p> <p><img src="/assets/img/blog_post/co/cod-116.png" alt="cod-116" width="90%"></p> <p>橙色表示 I/O Device 的信号，黑色表示 memory 的信号。Ack 相当于一条辅助信号线，用于告诉对方「我收到了你的请求 / 回应」。</p> <ol> <li> <p>When memory sees the ReadReq line, it reads the address from the data bus, begin the memory read operation，then raises Ack to tell the device that the ReadReq signal has been seen.</p> </li> <li> <p>I/O device sees the Ack line high and releases the ReadReq data lines.</p> </li> <li> <p>Memory sees that ReadReq is low and drops the Ack line.</p> </li> <li> <p>When the memory has the data ready, it places the data on the data lines and raises DataRdy.</p> </li> <li> <p>The I/O device sees DataRdy, reads the data from the bus , and signals that it has the data by raising ACK.</p> </li> <li> <p>The memory sees Ack signals, drops DataRdy, and releases the data lines.</p> </li> <li> <p>Finally, the I/O device, seeing DataRdy go low, drops the ACK line, which indicates that the transmission is completed.</p> </li> </ol> <p>【例题】</p> <blockquote> <p>Assume: <strong>The synchronous bus</strong> has a clock cycle time of 50 ns, and each bus transmission takes 1 clock cycle. <strong>The asynchronous bus</strong> requires 40 ns per handshake. The data portion of both buses is 32 bits wide.</p> <p>Question: Find the <strong>bandwidth</strong> for each bus when reading one word from a 200-ns memory.</p> </blockquote> <ul> <li> <p>Synchronous Case</p> <ol> <li>Send the address to memory : 50ns</li> <li>Read the memory : 200ns</li> <li>Send the data to the device : 50ns</li> </ol> <p>Thus, the total time is 300 ns. So, the bandwidth = 4bytes/300ns = 13.3MB/s</p> </li> <li> <p>Asynchronous Case</p> <p>回顾之前的握手协议图。</p> <p>Step 1: 40ns</p> <p>Step 2, 3, 4: $\max$(2 $\times$ 40ns + 40ns, 200ns) = 200ns</p> <p>Step 5, 6, 7: 3 $\times$ 40ns = 120ns</p> <p>所以读取一个 word 的总时间为 $360\text{ns}$。转换成带宽即为 $11.1$ MB/s。</p> </li> </ul> <h4 id="bus-arbitration">Bus Arbitration</h4> <p>当总线被多个 I/O Device 申请使用时，需要有人完成调度。一般承担这个工作的是 processor。</p> <p>四个常见的调度模式：</p> <ul> <li>daisy chain</li> <li>centralized</li> <li>self selection</li> <li>collision detection</li> </ul> <h4 id="bus-bandwidth-computation">Bus Bandwidth Computation</h4> <ul> <li> <p>例题</p> <p><img src="/assets/img/blog_post/co/cod-117.png" alt="cod-117" width="80%"></p> <p>Suppose we have a system with the following characteristic:</p> <ol> <li> <p>A memory and bus system supporting block access of (4~16) 32-bit words.</p> </li> <li> <p>A 64-bit synchronous bus clocked at 200 MHz, with each 64-bit transfer taking 1 clock cycle, and 1 clock cycle required to send an address to memory.</p> </li> <li> <p>Two clock cycles needed between each bus transaction.（每次利用总线读取一个 block 视作一个 transaction，每个 transaction 之间需要空 2 个周期）</p> </li> <li> <p>A memory access time for the first four words of 200ns; each additional set of four words can be read in 20 ns. Assume that a bus transfer of the most recently read data and a read of the next four words can be overlapped.</p> </li> </ol> <p>Q1. Find the <strong>sustained bandwidth</strong> and the latency for a read of 256 words for transfers that use <strong>4-word blocks</strong> and for transfers that use <strong>16-word blocks</strong>.</p> <p>Q2. Also compute <strong>effective number of bus transactions</strong> per second for each case.</p> </li> <li> <p>解答（4-word block case）</p> <p>For each block, it takes</p> <ol> <li> <strong>1 CC</strong> to send the address to memory</li> <li>200ns/(5ns/cycle) = <strong>40 CC</strong> to read memory</li> <li> <strong>2 CC</strong> to send the data from the memory</li> <li> <strong>2 CC</strong> needed between each bus operation.</li> </ol> <p>This is a total of <strong>45 CC</strong>.</p> <p>Since there are 256/4 = 64 blocks, the transfer of 256 words takes 45 $\times$ 64 = 2880 CC.</p> <p>The latency for the transfer of 256 words is: 2880 cycles $\times$ (5ns/cycle) = 14,400 ns.</p> <p>Final answer:</p> <ul> <li> <p>Number of bus transactions per second:</p> <p>$64 \text{ transactions} \times \frac{1 \text{second}}{14,400 \text{ns}} = 4.44 \text{M transactions/s}$.</p> <p>注意，当使用 4-word block 进行传输时，一次 256-word transfer 实际上包含了 64 次 bus transactions。</p> </li> <li> <p>Bandwidth</p> <p>$1024 \text{bytes} \times \frac{1 \text{second}}{14,400 \text{ns}} = 71.11 \text{ MB/s}$.</p> </li> </ul> </li> <li> <p>解答 (16-word block case)</p> <p>For each block, it takes</p> <ol> <li> <strong>1 CC</strong> to send the address to memory</li> <li>260ns/(5ns/cycle) = <strong>52 CC</strong> to read memory</li> <li> <strong>2 CC</strong> to send the data from the memory <strong>(Overlap Considered)</strong> </li> <li> <strong>2 CC</strong> needed between each bus operation.</li> </ol> <p>This is a total of <strong>57 CC</strong>.</p> <p>Since there are 256/16 = 16 blocks, the transfer of 256 words takes 57 $\times$ 16 = 912 CC.</p> <p>The latency for the transfer of 256 words is: 912 cycles $\times$ (5ns/cycle) = 4,560 ns.</p> <p>Final answer:</p> <ul> <li> <p>Number of bus transactions per second:</p> <p>$16 \text{ transactions} \times \frac{1 \text{second}}{4,560 \text{ns}} = 3.51 \text{M transactions/s}$.</p> </li> <li> <p>Bandwidth</p> <p>$1024 \text{bytes} \times \frac{1 \text{second}}{4,560 \text{ns}} = 224.56 \text{ MB/s}$.</p> </li> </ul> </li> </ul> <h3 id="interfacing-io-devices-to-the-memory-processor-and-os">Interfacing I/O Devices to the Memory, Processor, and OS</h3> <ul> <li> <p>I/O Characteristics</p> <ol> <li>(shared) 被多个程序共享</li> <li>(interrupts) 使用中断进行交互</li> <li>(complex) 底层控制非常复杂</li> </ol> </li> <li> <p>I/O Communication Types</p> <ol> <li>由 OS 发出指令给到 I/O devices（注意不是硬件）</li> <li>I/O devices 相应指令（无论是否成功完成操作）</li> <li>数据传输，必须发生在 I/O devices 和 memory 之中</li> </ol> </li> <li> <p>How to Give Commands to I/O Devices?</p> <p>【Method 1】Memory-Mapped I/O</p> <p>一种巧妙的设计，即每个 I/O 都与某个内存地址建立映射。当需要访问某个 I/O 时，直接用 <code class="language-plaintext highlighter-rouge">lw</code> 或 <code class="language-plaintext highlighter-rouge">sw</code> 等指令访问对应的 I/O 即可。</p> <p>【Method 2】Special I/O Instructions</p> <p>指令集为 I/O 专门设计的指令。例如 <code class="language-plaintext highlighter-rouge">in al, port</code> 和 <code class="language-plaintext highlighter-rouge">out port, al</code> 等。</p> </li> <li> <p>Communication with Processor</p> <ol> <li> <p><strong>Polling（轮寻）</strong>：定期（如每 10ms）去寻访某个 I/O，看其在过去的 10ms 内有无 I/O 请求。若有则执行。</p> </li> <li> <p><strong>Interrupt（中断）</strong>：当某个 I/O 发出请求，立刻中断 processor 并赶去执行。</p> <p><img src="/assets/img/blog_post/co/cod-118.png" alt="cod-118" width="80%"></p> <p>从上图可以看出，为了 incept data，printer 会抛出 request interrupt。</p> <p>而每当 printer 抛出一个 request interrupt，都会导致 CPU 产生一个中断（对应 response interrupt），并在这个中断中为 printer 传输数据。</p> </li> <li> <p><strong>Direct Memory Access (DMA)</strong>：</p> <p>实际上是 Interrupt 下的一个子分支，不过加入了 DMA 的优化。简单理解为，CPU 创建了一个专门处理 I/O 事件的「小跟班」DMA。</p> <p>以往 I/O Device 想要和 memory 交互（获取数据），必须要经过 CPU（这是因为 CPU 才能控制 memory）。而这会导致 CPU 被频繁中断，不好。</p> <p>优化后，在某一批次的 I/O 之前，CPU 事先创建并初始化一个 DMA，使其代为执行自己职责。这样只有在这一批 I/O 开始和结尾会导致 CPU 产生中断（共计 2 次，少了很多）。</p> <p><img src="/assets/img/blog_post/co/cod-119.png" alt="cod-119" width="60%"></p> </li> </ol> </li> <li> <p>Measure the Performance of Polling / Interrupt / DMA</p> <ol> <li> <p>Polling</p> <ul> <li> <p>Question:</p> <p><img src="/assets/img/blog_post/co/cod-120.png" alt="cod-120" width="60%"></p> </li> <li> <p>Solution:</p> </li> </ul> <p><img src="/assets/img/blog_post/co/cod-121.png" alt="cod-121" width="60%"></p> </li> <li> <p>Interrupt</p> <ul> <li> <p>Question:</p> <p>Suppose we have the same hard disk and processor we used in the former example, but we used interrupt-driven I/O. The overhead for each transfer, including the interrupt, is 500 clock cycles. Find the fraction of the processor consumed if <strong>the hard disk is only transferring data 5% of the time</strong>.</p> </li> <li> <p>Solution:</p> <p>实际上，最后一句话的假设是使得 Interrupt 在绝大多数情况下效率高于 Polling 的根本原因。考虑一个较长的时间段，比如说 1s。在此期间有 0.05s 我们进行 hard-disk I/O。这 0.05s 内我们需要传输 $0.05 \times 4 \text{MB} = 200 \text{KB}$ 数据。一次 data transfer 为 $16 \text{B}$，所以总共会造成 $12,500$ 次中断，损失 $12,500 \times 500 = 6.25 \times 10^6 \text{CC}$。</p> <p>一秒钟共 $500 \times 10^6 \text{CC}$，所以损失率为 $1.25\%$。</p> </li> </ul> </li> <li> <p>DMA</p> <ul> <li> <p>Question:</p> <p>Suppose we have the same hard disk and processor we used in the former example.</p> <p>Assume that the initial setup of a DMA transfer takes 1000 clock cycles for the processor, and assume the handling of the interrupt at DMA completion requires 500 clock cycles for the processor.</p> <p>The hard disk has a transfer rate of 4MB/sec and uses DMA. The average transfer from disk is 8 KB.<em>（这里的意思是，每次 transfer 的数据量大小 8KB。每次 transfer 之间可能相隔较久，所以需要重新初始化 DMA）</em> Assume the disk is actively transferring 100% of the time.</p> <p>Please find what fraction of the processor time is consumed.</p> </li> <li> <p>Solution:</p> <p>一次 disk transfer 所需时间为 8KB / (4MB / s) = 2ms。</p> <p>由于假设所有时间都在做 disk I/O，所以相当于 1s 内会做 500 批 disk I/O。</p> <p>每批 disk I/O 需要重新建立 DMA，同时打断 CPU，这部分总共消耗 1500 CC。</p> <p>所以 1s 内会使得 CPU 中断 $7.5 \times 10^5 \text{CC}$。损失率为 $0.2\%$。</p> </li> </ul> </li> </ol> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/IT-NOTE-2/">Information Theory Note (Part 2 of 2)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/IT-NOTE-1/">Information Theory Note (Part 1 of 2)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/CO-NOTE-3/">Computer Organization Note (Part 3 of 4)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/CO-NOTE-2/">Computer Organization Note (Part 2 of 4)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/CO-NOTE-1/">Computer Organization Note (Part 1 of 4)</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yuzhuo Tian. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>