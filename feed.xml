<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sqrtyz.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sqrtyz.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-30T03:16:10+00:00</updated><id>https://sqrtyz.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://sqrtyz.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://sqrtyz.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://sqrtyz.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Computer Organization Note (Part 1 of 4)</title><link href="https://sqrtyz.github.io/blog/2024/CO-NOTE-1/" rel="alternate" type="text/html" title="Computer Organization Note (Part 1 of 4)"/><published>2024-01-01T12:00:00+00:00</published><updated>2024-01-01T12:00:00+00:00</updated><id>https://sqrtyz.github.io/blog/2024/CO-NOTE-1</id><content type="html" xml:base="https://sqrtyz.github.io/blog/2024/CO-NOTE-1/"><![CDATA[<blockquote> <p>Part I 包含第一、三章，涉及导论内容和计算机运算问题。</p> </blockquote> <h2 id="chapter-1-computer-abstractions-and-technology">Chapter 1. Computer Abstractions and Technology</h2> <h3 id="history--development">History &amp; Development</h3> <ul> <li>第一代：电子管 / 真空管</li> <li>第二代：晶体管</li> <li>第三代：集成电路</li> <li>第四代：微处理器</li> </ul> <h3 id="computer-organization">Computer Organization</h3> <ul> <li> <p>Hardware</p> <p><img src="/assets/img/blog_post/co/cod-3.png" alt="cod-3"/></p> <p>Datapath 可以理解为一条「流水线」。</p> </li> <li> <p>Software</p> <p><img src="/assets/img/blog_post/co/cod-4.png" alt="cod-4"/></p> </li> </ul> <h3 id="指令集isa">指令集（ISA）</h3> <ul> <li> <p><strong>ISA</strong>: The interface between hardware and lowest-level software.</p> <p>指令集筑起了「硬件」与「软件」之间的桥梁。硬件生产厂商只需要支持指令集中的特定操作，而软件厂商只需要借助这些基本指令集来开发软件。</p> </li> </ul> <h3 id="cpu-time-calculation">CPU Time Calculation</h3> <ul> <li> <p>Basic Concepts</p> <ul> <li> <p><strong>[CONCEPT] Response Time</strong> (Execution Time)：响应时间，how long it takes to do a task.</p> </li> <li> <p><strong>[CONCEPT] Throughput</strong> (Bandwidth)：吞吐率，total work done per unit time.</p> </li> </ul> </li> <li> <p>Performance Measurement</p> <p>假设 X 比 Y 快 $n$ 倍，那么 $\frac{\text{Execution Time}_Y}{\text{Execution Time}_X}=n$。</p> </li> <li> <p>Clock</p> <p><img src="/assets/img/blog_post/co/cod-5.png" alt="cod-5"/></p> <ul> <li> <p><strong>[CONCEPT] Clock Cycle Time</strong>: duration of a clock cycle.</p> </li> <li> <p><strong>[CONCEPT] Clock Rate</strong>: cycles per second，与 Clock period 成倒数。</p> </li> <li> <p>CPU Time = CPU Clock Cycles × Clock Cycle Time = CPU Clock Cycles / Clock Rate</p> </li> </ul> </li> <li> <p>Considering Instructions</p> <ul> <li> <p><strong>[CONCEPT] CPI</strong>: Cycles Per Instruction. <strong>CPI 的值由 CPU 本身决定，同时也受到指令种类的影响</strong>（例如，加法和乘法所需要的 Cycles 肯定不一样）。</p> </li> <li> <p>Clock Cycles = Instruction Count × CPI</p> <p>CPU Time = Instruction Count × CPI × Clock Cycle Time = Instruction Count × CPI / Clock Rate</p> </li> <li> <p>考虑不同 Instruction Type 对 CPI 的影响……</p> <p>$\text{Clock Cycles} = \sum\limits_{i=1}^n (\text{CPI}_i \times \text{Instruction Count}_i)$</p> <p>加权平均 CPI（global CPI）：字面意思。</p> </li> </ul> </li> <li> <p>Power Spent</p> <p>$\text{Power} = \text{Capacitive load}^2 \times \text{Voltage} \times \text{Frequency}$</p> </li> <li> <p>习题演示</p> <p><img src="/assets/img/blog_post/co/cod-6.png" alt="cod-6"/></p> </li> </ul> <h3 id="some-pitfalls--fallacies">Some Pitfalls / Fallacies</h3> <ul> <li> <p>Amdahl’s Law 提升局部不一定能提升总体</p> \[T_{imporved} = \frac{T_{affected}}{\text{improvement factor}} + T_{unaffected}\] <p>其中 $T_{imporved}$ 表示提升后的总用时。$\text{improvement factor}$ 表示受影响的提升方面的提升比例。此公式意在说明，对于某种提升，只有 affected 的部分会变快，unaffected 的部分怎么都不会变快。</p> <p>例如，一个程序需要执行 80s 的乘法和 20s 的除法。无论乘法提升了多少，我们都无法把总时间提升 5 倍。</p> </li> <li> <p>Low Power at Idle 功耗和负载不成正比</p> <p>对于 i7 power benchmark，100% 负载下的功耗为 258W，50% 负载下为 170W，10% 负载下为 121W。也就是说，一味降低 CPU 的负载不一定能起到节能的效果。</p> </li> <li> <p>MIPS as a Performance Metric? MIPS 是否能作为衡量 CPU 运行速度的参数？</p> <p>MIPS: Millions of Instructions Per Second</p> \[\text{MIPS} = \frac{\text{Clock Rate}}{\text{CPI} \times 10^6}\] <p>之前提及，CPI 除了受 CPU 本身的影响，也受到 CPI 种类等因素的影响。不同电脑的指令集架构不同，进而导致 CPI 不同；同时，不同指令的 CPI 也不一样。所以 MIPS 并非一个只和 CPU 本身有关的量。所以用 MIPS 来衡量 CPU 的运行速度是不合理的。</p> </li> </ul> <h3 id="eight-great-ideas">Eight Great Ideas</h3> <ul> <li> <p>Design for Moore’s Law (设计紧跟摩尔定律)</p> </li> <li> <p>Use Abstraction to Simplify Design (采用抽象简化设计)</p> </li> <li> <p>Make the Common Case Fast (加速大概率事件)</p> </li> <li> <p>Performance via Parallelism (通过并行提高性能)</p> </li> <li> <p>Performance via Pipelining (通过流水线提高性能)</p> </li> <li> <p>Performance via Prediction (通过预测提高性能)</p> </li> <li> <p>Hierarchy of Memories (存储器层次)</p> </li> <li> <p>Dependability via Redundancy (通过冗余提高可靠性)</p> </li> </ul> <hr/> <ul> <li> <p>Some illustrations:</p> <p>Use Abstraction to Simplify Design:</p> <p><img src="/assets/img/blog_post/co/cod-1.png" alt="cod-1"/></p> <p>Performance via Pipelining:</p> <p><img src="/assets/img/blog_post/co/cod-2.png" alt="cod-2"/></p> </li> </ul> <h2 id="chapter-3-arithmetic-for-computer">Chapter 3. Arithmetic for Computer</h2> <h3 id="signed-numbers-representations">Signed Numbers Representations</h3> <table> <thead> <tr> <th>Binary</th> <th>Signed Magnitude</th> <th>2’s Complement</th> </tr> </thead> <tbody> <tr> <td>000</td> <td>+0</td> <td>+0</td> </tr> <tr> <td>001</td> <td>+1</td> <td>+1</td> </tr> <tr> <td>010</td> <td>+2</td> <td>+2</td> </tr> <tr> <td>011</td> <td>+3</td> <td>+3</td> </tr> <tr> <td>100</td> <td>-0</td> <td>-4</td> </tr> <tr> <td>101</td> <td>-1</td> <td>-3</td> </tr> <tr> <td>110</td> <td>-2</td> <td>-2</td> </tr> <tr> <td>111</td> <td>-3</td> <td>-1</td> </tr> </tbody> </table> <p>一般采取补码的表示方式。</p> <h3 id="addition-and-subtraction">Addition and Subtraction</h3> <p>减法可以改成加补码。这样加和减都统一成了加法。</p> <p>加减法都可能产生 overflow。例如：</p> <p><img src="/assets/img/blog_post/co/cod-7.png" alt="cod-7"/></p> <p>overflow 的判定遵循下表：</p> <p><img src="/assets/img/blog_post/co/cod-8.png" alt="cod-8"/></p> <ul> <li> <p>此处的 overflow 专指「溢出导致错误的计算结果」。例如 $(-1)+(-1)=(-2)$ 我们不认为发生了 overflow。</p> </li> <li> <p>只有以上四种 A, B 组合才可能产生 overflow。例如一个正数加一个负数是不可能产生 overflow 的。</p> </li> <li> <p>Result overflow 括号中的两个数分别表示「溢出位」和「最高位」。例如 $7+7=(0111)_2+(0111)_2=(1110)_2=-2$，其溢出位（第 4 位）为 $0$，最高位（第 3 位）为 $1$，属于表中的第一行情况。</p> </li> <li> <p>可以看到「溢出位」和「最高位」在溢出情况下总是相反的，这意味着我们可以使用 XOR 门进行判定。</p> </li> </ul> <h3 id="alu">ALU</h3> <p>考虑建立具有如下功能的 ALU：</p> <p><img src="/assets/img/blog_post/co/cod-9.png" alt="cod-9"/></p> <p>其中 srl 表示 shift right logical，Zero 位宽为 1，输出 1 当且仅当 Result 为 0。</p> <ol> <li> <p>And 与 Or</p> <p><img src="/assets/img/blog_post/co/cod-10.png" alt="cod-10"/></p> </li> <li> <p>Add</p> <p>全加器，$S=a \oplus b \oplus C_{in}, \ C_{out}=ab+aC_{in}+bC_{in}$。</p> <p><img src="/assets/img/blog_post/co/cod-11.png" alt="cod-11"/></p> </li> <li> <p>Sub</p> <p>减法就是加上补码。考虑 And / Or / Add / Sub 的 ALU 设计如下：</p> <p><img src="/assets/img/blog_post/co/cod-12.png" alt="cod-12"/></p> </li> <li> <p>Set on less than (Comparison)</p> <p>用于比较输入项 $a,b$ 的大小，如果 $a &lt; b$，那么输出 $1$，反之输出 $0$。</p> <p>实现方法是执行减法运算，看最高位（符号位）是否为 $1$ 即可。</p> </li> </ol> <p>考虑 And / Or / Add / Sub / Set on less than 的 ALU 设计如下：</p> <p><img src="/assets/img/blog_post/co/cod-13.png" alt="cod-13"/></p> <p>其中 MSB 的设计有略微不同（加入了溢出检测模块，输出 Set 和 Overflow 信号）</p> <p><img src="/assets/img/blog_post/co/cod-14.png" alt="cod-14"/></p> <p><strong>最后我们考虑建立整个 ALU。</strong></p> <p><img src="/assets/img/blog_post/co/cod-15.png" alt="cod-15"/></p> <p>注意到 MSB 的 Set 连到了 LSB 的 Less 输入，用于执行 Comparison 运算；非 LSB 的 Less 输入均为零。</p> <h3 id="fast-adder">Fast Adder</h3> <p><strong>Carry Lookahead Adder (CLA) 加速运算</strong>：本质上是把行波加法给「压缩」了。但受到 fan-in of gate 的影响，只有分组「压缩」。</p> <p>此部分数逻已经学过，在此不再赘述。</p> <h3 id="multiplication">Multiplication</h3> <p><strong>名词</strong>：被乘数（multiplicand），乘数（multiplier），积（product）。</p> <ol> <li> <p>Multiplier Ver 1</p> <p>Look at current bit position.</p> <ul> <li> <p>if multiplier is 1, then add multiplicand.</p> </li> <li> <p>Otherwise add 0.</p> </li> <li> <p>Shift multiplicand left by 1 bit.</p> </li> </ul> <p><img src="/assets/img/blog_post/co/cod-16.png" alt="cod-16"/></p> <p>其实就是模拟的竖式乘法。对于两个 64 bits 的整数相乘，需要 128 bits 的寄存器存储积，同时也需要一个 128 bits 的 ALU 来执行加法。</p> </li> <li> <p>Multiplier Ver 2</p> <p>在第一版乘法器的基础上，第二版将「左移 multiplicand」改成了「右移 product」来减小 ALU 的开销。原理图和举例图如下。</p> <p><img src="/assets/img/blog_post/co/cod-17.png" alt="cod-17"/></p> <p><img src="/assets/img/blog_post/co/cod-18.png" alt="cod-18"/></p> <p>注意每一次是将 multiplicand 加到 product 左边的 64 位，而后进行不断的右移。对于两个 64 bits 的整数相乘，需要 128 bits 的寄存器存储积，但只需要一个 64 bits 的 ALU 来执行加法。</p> </li> <li> <p>Multiplier Ver 3</p> <p>基于第二版乘法器进行了一点小优化。回顾我们使用的 128 位的存储积的 reg，刚开始 reg 的右 64 位是空的，这意味着说我们可以将其用于存储 multiplier。并且随着后续 product 和 multiplier 的右移，两者也不会产生空间上的冲突。</p> <p><img src="/assets/img/blog_post/co/cod-19.png" alt="cod-19"/></p> <p><img src="/assets/img/blog_post/co/cod-20.png" alt="cod-20"/></p> <p>红色部分存储的是 multiplier。可以看到 multiplier 和 product 共用一个 reg。</p> </li> <li> <p>有符号乘法</p> <p>考虑有符号整数乘法。一种基本的方法是，符号和绝对值分开考虑。符号位直接进行 XOR，随后将 MSB 设为 0（去除符号位）并执行无符号乘法，最后将符号和绝对值的积合并即可。</p> <p>此外还有一种方法：<strong>Booth’s Algorithm.</strong></p> </li> <li> <p>Booth 算法</p> <p>Booth 算法的执行基于第三版乘法器，但是做了一定的优化。<strong>Booth 算法在加速了乘法的同时，也使得有符号的乘法运算更加统一（即不需要传统方法的分类讨论）。</strong></p> <p>考虑被乘数为 $y$，乘数为 $10111100$。按照第三版乘法器的做法，会执行 $8$ 次 shift 操作和 $5$ 次 add 操作。如果我们将 $10111100$ 改写为 $2^7+2^6-2^2$，则只需要执行 $8$ 次 shift 操作和 $3$ 次 add/sub 操作（在此我们假设 shift 速度快于 add/sub）。</p> <ul> <li> <p>流程</p> <p>每次考察乘数的 0 位和 -1 位（此处 -1 位是人为补足的，初始为 0）。并按照以下表格执行操作：</p> <table> <thead> <tr> <th>bit(0) and bit(-1)</th> <th>Operations</th> </tr> </thead> <tbody> <tr> <td>$10$</td> <td>Subtract multiplicand from left half</td> </tr> <tr> <td>$11$</td> <td>No arithmetic operations, just shift right</td> </tr> <tr> <td>$01$</td> <td>Add multiplicand to left half</td> </tr> <tr> <td>$00$</td> <td>No arithmetic operations, just shift right</td> </tr> </tbody> </table> <p>此处依然是按照第三版乘法器那样将 product, multiplier 放在一起存储，且每次 shift right 都是将两者一起右移。</p> <p><strong>同时：每次右移最高位的不足不再是无脑补零，而是和原最高位一样补（即每次右移保持符号位不变）。</strong></p> </li> <li> <p>例子</p> <p><img src="/assets/img/blog_post/co/cod-22.png" alt="cod-22"/></p> <p>注意执行的轮次数和乘数的 bit 数一致。第一轮是 10，第二轮是 01，第三轮是 10，第四轮是 11，第五轮（本来是 11）无需执行。</p> </li> <li> <p>一些粗浅的理解</p> <p><strong>关于加减法：</strong> 遇 10 则减，遇 01 则加其实比较好颅内理解。00 和 11 不干事都是为了「把锅甩给后面的高位」。</p> <p><strong>关于右移最高位补足、为何不进行第 5 轮：</strong> 首先考虑另外一个问题，为什么常规无符号乘法无法做有符号的乘法。例如，$0010 \times 1101 = 00011010$，正负得正，显然是有问题的。</p> <p>实际上问题出在位数不足。补齐后 $00000010 \times 11111101 = 11111010$ （取低 8 位）就没有问题了。</p> <p>然而把四位乘法转化为 8 bits × 8 bits 做是有点亏的，同时注意到有很多连续的 1 似乎加法也会很多。所以使用 Booth’s Algorithm。</p> <p>解答这个问题可能需要考虑 8 bits × 8 bits 的本原做法。比如在 iteration 1 中要减去被乘数，在算法中体现的是 +1110，然而实际上应该是 +11111110。这样在整体右移一位之后最高位确实应该为 1。</p> <p>为何不进行最后一轮似乎也和这个有关。像比如上图的例子，按理说最后应该再做一个 left half 的 +0010 才对，然而当我们把乘数 $1101$ 补全为 $1111 \ 1101$ 后，发现实际上这个 +0010 应该在 iter 9 执行，实际上表现出来的就是溢出了 8 bits 的范围。</p> </li> </ul> </li> </ol> <h3 id="division">Division</h3> <p><strong>名词</strong>：被除数（dividend），除数（divisor），商（quotient），余数（remainder）。</p> <ol> <li> <p>Division Introduction</p> <ul> <li> <p>除数为 0 一般交由软件判断。</p> </li> <li> <p>有符号除法一般将符号和绝对值分开讨论。</p> </li> </ul> </li> <li> <p>Division Ver 1</p> <p><img src="/assets/img/blog_post/co/cod-23.png" alt="cod-23"/></p> <p>假设 dividend 和 divisor 都是 64 位的。</p> <ul> <li> <p>首先将实际的 divisor 放到高 64 位（低 64 位为零）。remainder 初始设置为 dividend。</p> </li> <li> <p>接下来每次拿 remainder 减去 divisor。若减后 remainder 大于等于 0，则将 quotient 左移并将 LSB 设为 1；若减后 remainder 小于 0，则恢复原值（把 divisor 加回去），同时也将 quotient 左移。</p> </li> <li> <p>将 divisor 右移一位。重复 65 次。</p> </li> </ul> <p>其实就是模拟的竖式除法。一个示例如下。</p> <p><img src="/assets/img/blog_post/co/cod-24.png" alt="cod-24"/></p> </li> <li> <p>Division Ver 3</p> <p>和之前的 Multiplier V3 一致的思路：将 remainder 和 quotient 放在一个 reg 中存放。</p> <p><img src="/assets/img/blog_post/co/cod-25.png" alt="cod-25"/></p> <p><img src="/assets/img/blog_post/co/cod-26.png" alt="cod-26"/></p> <p>注意初始的时候就可以将 remainder 左移一位（对应 iter0，因为第一次必然失败）。最后 reg 中的 left half 即为 remainder，right half 即为 quotient。</p> </li> </ol> <h3 id="float">Float</h3> <ul> <li> <p>浮点数的存储与定义</p> <p>浮点数的存储分为三部分。</p> <ul> <li>float：符号位 $S$ (1bit)，指数位 $E$ (8bits)，系数位 $F$ (23bits)。</li> <li>double：符号位 $S$ (1bit)，指数位 $E$ (11bits)，系数位 $F$ (52bits)。</li> </ul> <p>其意义为二进制下的科学计数法表示：$(-1)^S \times 1.F \times 2^{E+bias}$。</p> <p>实际使用时指数位 $E$ 会有偏差 $bias$，这是为了处理 $E$ 可能为负的情况。对于 float，$E$ 存储值比实际值大 $bias=127$；对于 double，$E$ 存储值比实际值大 $bias=1023$。</p> <p>特别规定：</p> <ul> <li>当 $E=111…11, \ F=000…00$，表示此浮点数为无穷。</li> <li>当 $E=111…11, \ F \neq 000…00$，表示此浮点数为 NaN。</li> </ul> </li> <li> <p>浮点数加法</p> <p>算法流程与示例：</p> <ol> <li> <p>比较两数的指数位，确定大小关系并把指数位较小的通过 $F$ 右移（小数点则向左移动）使得两操作数的指数位 $E$ 一致。</p> </li> <li> <p>将 $F$ 相加。</p> </li> <li> <p>归一化（比如加法之后产生了进位，那么 $E$ 应当加一，$F$ 则右移）。</p> </li> <li> <p>Rounding（比如四舍五入）</p> </li> <li> <p>再次归一化（例如 9.99 round 后变为 10.00，此时需要再次归一化，当然这只是一个十进制的例子）</p> </li> </ol> <p><img src="/assets/img/blog_post/co/cod-27.png" alt="cod-27"/></p> <p>逻辑结构图绘制：</p> <p><img src="/assets/img/blog_post/co/cod-28.png" alt="cod-28"/></p> </li> <li> <p>浮点数乘法</p> <p>区别主要集中在：</p> <ol> <li>符号位单独处理</li> <li>指数位 $E$ 相加后要减去对应的 $bias$。</li> </ol> <p><img src="/assets/img/blog_post/co/cod-29.png" alt="cod-29"/></p> </li> <li> <p>Accurate Arithmetic 浮点数精度问题</p> <p>一些术语：</p> <ul> <li> <p>保留位 (guard bit)：第一个被舍掉的位。</p> </li> <li> <p>近似位 (round bit)：guard bit 的后面一位，亦即第二个被舍掉的位。</p> </li> <li> <p>粘滞位 (sticky bit)：round bit 后面的所有位的 OR 构成 sticky bit。</p> </li> </ul> <p>Rounding 规则如下：</p> <p><img src="/assets/img/blog_post/co/cod-30.png" alt="cod-30"/></p> </li> </ul>]]></content><author><name></name></author><category term="Course"/><category term="Notes"/><category term="Hardware"/><summary type="html"><![CDATA[Contains content about CO introduction and arithmetic knowledge.]]></summary></entry><entry><title type="html">Computer Organization Note (Part 2 of 4)</title><link href="https://sqrtyz.github.io/blog/2024/CO-NOTE-2/" rel="alternate" type="text/html" title="Computer Organization Note (Part 2 of 4)"/><published>2024-01-01T12:00:00+00:00</published><updated>2024-01-01T12:00:00+00:00</updated><id>https://sqrtyz.github.io/blog/2024/CO-NOTE-2</id><content type="html" xml:base="https://sqrtyz.github.io/blog/2024/CO-NOTE-2/"><![CDATA[<h2 id="chapter-2-instructions-language-of-the-machine">Chapter 2. Instructions: Language of the Machine</h2> <h3 id="introduction">Introduction</h3> <ul> <li> <p>Process of Compiling</p> <p>编译过程：高级编程语言 $\to$ 汇编语言 $\to$ 机器语言。</p> <p>例如，<code class="language-plaintext highlighter-rouge">A + B</code> $\to$ <code class="language-plaintext highlighter-rouge">add A, B</code> $\to$ <code class="language-plaintext highlighter-rouge">1000110010100000</code>。</p> </li> <li> <p>Instruction</p> <p>指令是「机器的语言」。如果说 Instruction 是 Word，那么 Instruction set 则是 Vocabulary。</p> <p>本课程采取的指令集为 RISC-V。这是一种精简指令集。</p> <p>关于汇编语言和指令集的关系，摘取两条知乎回答：</p> <blockquote> <p>汇编语言是用人类看得懂的语言来描述指令集。否则指令集的机器码都是一堆二进制数字，人类读起来非常麻烦，但汇编是用类似人类语言的方式描述指令集，读起来方便多了。</p> </blockquote> <blockquote> <p>汇编指令是让人看得懂的，只是指令集的另外一种表示形式。</p> </blockquote> </li> <li> <p>RISC vs. CISC</p> <p><img src="/assets/img/blog_post/co/cod-31.png" alt="cod-31"/></p> </li> </ul> <h3 id="basics-instructions-and-registers">Basics: Instructions and Registers</h3> <ul> <li> <p>Instruction Formats</p> <p><img src="/assets/img/blog_post/co/cod-32.png" alt="cod-32"/></p> <p>前者是可以理解为操作类型，后者可以理解为操作参数。</p> </li> <li> <p>Register Operands</p> <p>对于 RISC-V 指令集，其使用的寄存器类型为 $32\times 64\text{-bit}$。可以想象成一个寄存器有 32 行，每一行有一个变量，其长度为 64 bits（等于 8 bytes，是一个 double / long long 的大小，也被称为 dword）。</p> <p>这些寄存器的「各行」各司其职。其职能如下表所示。</p> <p><img src="/assets/img/blog_post/co/cod-33.png" alt="cod-33"/></p> <p>举个例子：<code class="language-plaintext highlighter-rouge">f = (g + h) - (i + j);</code>，我们假设目前 $f,g,h,i,j$ 的值分别位于地址 x19, x20, x21, x22, x23。</p> <p>那么我们应该这样描述 RISC-V 指令：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  add x5, x20, x21
  add x6, x22, x23
  sub x19, x5, x6
</code></pre></div> </div> </li> </ul> <h3 id="al-1-arithmetic-logical-and-memory">AL 1: Arithmetic, Logical and Memory</h3> <ul> <li> <p>Add and Subtraction</p> <table> <thead> <tr> <th>Category</th> <th>Instruction</th> <th>Example</th> <th>Meaning</th> <th>Comments</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>Arithmetic</td> <td>add</td> <td><code class="language-plaintext highlighter-rouge">add a,b,c</code></td> <td>$a \gets b+c$</td> <td>Always 3 operand</td> <td>R</td> </tr> <tr> <td>Arithmetic</td> <td>subtract</td> <td><code class="language-plaintext highlighter-rouge">sub a,b,c</code></td> <td>$a \gets b-c$</td> <td>Always 3 operand</td> <td>R</td> </tr> </tbody> </table> </li> <li> <p>Immediate Arithmetic</p> <table> <thead> <tr> <th>Category</th> <th>Instruction</th> <th>Example</th> <th>Meaning</th> <th>Comments</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>Arithmetic</td> <td>addi immediate</td> <td><code class="language-plaintext highlighter-rouge">addi a,b,c</code></td> <td>$a \gets b+c$</td> <td>$c$ is a constant</td> <td>I</td> </tr> </tbody> </table> </li> <li> <p>Logical Arithmetic</p> <p><img src="/assets/img/blog_post/co/cod-39.png" alt="cod-39"/></p> <p><img src="/assets/img/blog_post/co/cod-40.png" alt="cod-40"/></p> <p>（懒得敲表格了，直接截图）</p> </li> <li> <p>Memory Operations</p> <ul> <li> <p>Basics</p> <ol> <li>执行算术操作：先从 memory 中读到 reg，在 reg 中进行运算，最后输回到 reg。</li> <li>Memory 的地址是 <strong>按 byte 定义的</strong>。</li> <li>RISC-V 采取小端。</li> </ol> </li> <li> <p>Example</p> <p>考虑 C 代码：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">A</span><span class="p">[</span><span class="mi">12</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="mi">8</span><span class="p">];</span> <span class="c1">// here A is a double</span>
</code></pre></div> </div> <p>假设 h 在 reg 中的值位于 x21，A 在 memory 中的地址值保存在 reg 中的 x22。</p> <p>用 RISC-V 表示：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ld x9, 64(x22)
  add x9, x21, x9
  sd x9, 96(x22)
</code></pre></div> </div> <ol> <li>第一行：由于 A 是 double 类型，所以 A[8] 相对于 A 的偏移为 64。<code class="language-plaintext highlighter-rouge">ld</code> 用于从 mem 读到 reg，使用 <code class="language-plaintext highlighter-rouge">ld x9, 64(x22)</code> 将 A 地址向后偏移 64 bytes 处的值读入到 reg 的 x9 中。</li> <li>第二行：执行加法。</li> <li>第三行：将 reg 的 x9 中的值输出到 mem 中。具体位置是 x22 所存地址向后偏移 96。</li> </ol> <table> <thead> <tr> <th>Category</th> <th>Instruction</th> <th>Example</th> <th>Meaning</th> <th>Comments</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>Data Transfer</td> <td>load dword</td> <td><code class="language-plaintext highlighter-rouge">ld x5, 40(x6)</code></td> <td>x5 = Mem[x6+40]</td> <td>dword from mem to reg</td> <td>I</td> </tr> <tr> <td>Data Transfer</td> <td>store dword</td> <td><code class="language-plaintext highlighter-rouge">sd x5, 40(x6)</code></td> <td>Mem[x6+40] = x5</td> <td>dword from reg to mem</td> <td>S</td> </tr> </tbody> </table> </li> </ul> </li> <li> <p>Sign Extension</p> <p>Example: 8-bit to 16-bit</p> <p>+2: 0000 0010 =&gt; 0000 0000 0000 0010</p> <p>-2: 1111 1110 =&gt; 1111 1111 1111 1110</p> <p>在 RISC-V 中，<code class="language-plaintext highlighter-rouge">lb</code> 操作用于执行「同符号位拓展」，<code class="language-plaintext highlighter-rouge">lbu</code> 操作用于执行「补 0 位拓展」。</p> </li> </ul> <h3 id="ml-1-representing-instructions-of-r--i--s">ML 1: Representing Instructions of R / I / S</h3> <ul> <li> <p>汇编码 $\to$ 机器码</p> <p>一个例子如下：</p> <p><img src="/assets/img/blog_post/co/cod-34.png" alt="cod-34"/></p> <p>可以看到，汇编码向机器码的转换已经很好理解了。更具体一点，它的规则如下：</p> <p><img src="/assets/img/blog_post/co/cod-35.png" alt="cod-35"/></p> <p>一条指令的长度恒为 32 位。注意构成 operator 的不止有 opcode，还有 funct7 和 funct3。例如，<code class="language-plaintext highlighter-rouge">add</code> 和 <code class="language-plaintext highlighter-rouge">sub</code> 的 opcode 实际上是相同的。</p> <p>实际上，这种机器码指令为 R 型指令，他只是众多机器码指令类型中的一种。</p> <p><img src="/assets/img/blog_post/co/cod-36.png" alt="cod-36"/></p> <ul> <li> <p>R-Format Instructions</p> <p>主要包含 arithmetic。格式和意义如上所示。</p> </li> <li> <p>I-Format Instructions</p> <p>主要包含 immediate arithmetic 和 load instructions。</p> <p><img src="/assets/img/blog_post/co/cod-37.png" alt="cod-37"/></p> </li> <li> <p>S-Format Instructions</p> <p>主要包含 store instructions。</p> <p><img src="/assets/img/blog_post/co/cod-38.png" alt="cod-38"/></p> </li> </ul> </li> </ul> <h3 id="al-2-decision-instructions">AL 2: Decision Instructions</h3> <p>基本 RISC-V 语句：beq &amp; bne</p> <table> <thead> <tr> <th>Category</th> <th>Instruction</th> <th>Example</th> <th>Meaning</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>Decision</td> <td>equal</td> <td><code class="language-plaintext highlighter-rouge">beq rs1, rs2, L1</code></td> <td>if (<code class="language-plaintext highlighter-rouge">rs1 == rs2</code>) branch to instruction labeled L1</td> <td>SB</td> </tr> <tr> <td>Decision</td> <td>not equal</td> <td><code class="language-plaintext highlighter-rouge">bne rs1, rs2, L1</code></td> <td>if (<code class="language-plaintext highlighter-rouge">rs1 != rs2</code>) branch to instruction labeled L1</td> <td>SB</td> </tr> <tr> <td>Decision</td> <td>less than</td> <td><code class="language-plaintext highlighter-rouge">blt rs1, rs2, L1</code></td> <td>if (<code class="language-plaintext highlighter-rouge">rs1 &lt; rs2</code>) branch to instruction labeled L1</td> <td>SB</td> </tr> <tr> <td>Decision</td> <td>greater than</td> <td><code class="language-plaintext highlighter-rouge">bge rs1, rs2, L1</code></td> <td>if (<code class="language-plaintext highlighter-rouge">rs1 &gt;= rs2</code>) branch to instruction labeled L1</td> <td>SB</td> </tr> </tbody> </table> <p>Unsigned comparison: <code class="language-plaintext highlighter-rouge">bltu</code>, <code class="language-plaintext highlighter-rouge">bgeu</code>（在后面加 u 变成无符号）</p> <ul> <li> <p>单 if 情况</p> <p>考虑 C 代码：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">)</span> <span class="k">goto</span> <span class="n">L1</span><span class="p">;</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">h</span><span class="p">;</span>
  <span class="n">L1</span><span class="o">:</span> <span class="n">f</span> <span class="o">=</span> <span class="n">f</span> <span class="o">-</span> <span class="n">i</span><span class="p">;</span>
</code></pre></div> </div> <p>对应的 RISC-V 汇编代码：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  beq x21, x22, L1      # go to L1 if i equals j
  add x19, x20, x21     # f = g + h ( skipped if i equals j )
  L1: sub x19, x19, x22 # f = f - i ( always executed )
</code></pre></div> </div> </li> <li> <p>if-else 情况</p> <p>考虑 C 代码：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="n">j</span><span class="p">)</span> <span class="n">f</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">h</span><span class="p">;</span>
  <span class="k">else</span> <span class="n">f</span> <span class="o">=</span> <span class="n">g</span> <span class="o">-</span> <span class="n">h</span><span class="p">;</span>
</code></pre></div> </div> <p>对应的 RISC-V 汇编代码（Assume that f~j are located at x19~x23）：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  bne x22, x23, Else  # go to Else if i != j
  add x19, x20, x21  # f = g + h ( Executed if i == j )
  beq x0, x0, EXIT  # always go to Exit
  Else: sub x19, x20, x21  # f = g - h ( Executed if i ≠ j ) 
  Exit:  # the first instruction of the next C 
</code></pre></div> </div> </li> <li> <p>LOOPs 情况</p> <p>考虑 C 代码：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nl">Loop:</span> <span class="n">g</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">];</span> <span class="c1">// A is an array of 100 words</span>
  <span class="n">i</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">j</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">h</span> <span class="p">)</span> <span class="k">goto</span> <span class="n">Loop</span><span class="p">;</span>
</code></pre></div> </div> <p>对应的 RISC-V 汇编代码（Assume that f~j are located at x19~x23, base of A is stored in x25）：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Loop: slli x10, x22, 3 # temp reg x10 = 8 * i
  add x10, x10, x25      # x10 = address of A[i]
  ld x19, 0(x10)         # temp reg x19 = A[i]
  add x20, x20, x19      # g = g + A[i]
  add x22, x22, x23      # i = i + j
  bne x22, x21, Loop     # go to Loop if i != h
</code></pre></div> </div> <p>第一行 <code class="language-plaintext highlighter-rouge">x22</code> 乘以 $8$ 之后放入 <code class="language-plaintext highlighter-rouge">x10</code>，和第二行共同计算出了 $A_i$ 的内存地址，置于 x10。</p> <p>第三行将 $A_i$ 的值读入到 <code class="language-plaintext highlighter-rouge">x19</code>。注意此处 <code class="language-plaintext highlighter-rouge">0(x10)</code> 中 <code class="language-plaintext highlighter-rouge">0</code> 只能是常量，因而不能用之前的那种寻址方式。</p> </li> <li> <p>while 情况</p> <p>考虑 C 代码：</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">while</span> <span class="p">(</span><span class="n">save</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span>
      <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span><span class="p">;</span>
</code></pre></div> </div> <p>对应的 RISC-V 汇编代码（Assume i~k are located at x22~x24, base of save is stored in x25）：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Loop: slli x10, x22, 3 # temp reg $t1 = 8 * i
  add x10, x10, x25      # x10 = address of save[i]
  ld x9, 0(x10)          # x9 gets save[i]
  bne x9, x24, Exit      # go to Exit if save[i] != k
  addi x22, x22, 1       # i += 1
  beq x0, x0, Loop       # go to Loop 
  Exit:
</code></pre></div> </div> </li> <li> <p>比较</p> <p>使用 <code class="language-plaintext highlighter-rouge">blt</code>, <code class="language-plaintext highlighter-rouge">bge</code> 表示变量比较形式的选择。例如 <code class="language-plaintext highlighter-rouge">blt rs1, rs2, L1</code> 表示如果 rs1 &lt; rs2 则跳到 L1 标签处的指令。</p> <p>注意 <code class="language-plaintext highlighter-rouge">bltu</code>, <code class="language-plaintext highlighter-rouge">bgeu</code> 表示无符号比较。使用这种比较方式可能会导致比较结果不同。</p> <p><img src="/assets/img/blog_post/co/cod-41.png" alt="cod-41"/></p> </li> </ul> <h3 id="al-3-jump-register">AL 3: Jump Register</h3> <p>RISC-V 中的无条件跳转主要包含两种指令：<code class="language-plaintext highlighter-rouge">jal</code> 和 <code class="language-plaintext highlighter-rouge">jalr</code>。</p> <table> <thead> <tr> <th>Category</th> <th>Instruction</th> <th>Example</th> <th>Meaning</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>Unconditional branch</td> <td>jump and link</td> <td><code class="language-plaintext highlighter-rouge">jal x1, 100</code></td> <td>x1 = PC + 4; go to PC+100</td> <td>UJ</td> </tr> <tr> <td>Unconditional branch</td> <td>jump and link register</td> <td><code class="language-plaintext highlighter-rouge">jalr x1, 100(x5)</code></td> <td>x1 = PC + 4; go to x5+100</td> <td>I</td> </tr> </tbody> </table> <p>这之中 PC 的意思是当前指令的地址。实际上，所有指令都存储在内存之中（后面会再讲）。这里跳实际上也是一种「在内存里跳」。注意每个指令的长度为 32 bits。</p> <p><code class="language-plaintext highlighter-rouge">jal</code> 可以理解为相对位置跳转，<code class="language-plaintext highlighter-rouge">jalr</code> 可以理解为绝对位置跳转。</p> <ul> <li> <p>使用 Jump address table 来指导跳转</p> <p>考虑这样的 C 代码，其中 $f\sim k$ 存储在 x20 ~ x25 中。</p> <pre><code class="language-C">  switch ( k ) {
      case 0 : f = i + j ; break ; /* k = 0 */
      case 1 : f = g + h ; break ; /* k = 1 */
      case 2 : f = g - h ; break ; /* k = 2 */
      case 3 : f = i - j ; break ; /* k = 3 */
  }
</code></pre> <p>其 RISC-V 指令如下：</p> <p><img src="/assets/img/blog_post/co/cod-42.png" alt="cod-42"/></p> <p>从内存中的 x6 开始一段，存储着我们用来辅助跳转的 Jump address table。第四行根据 $k$ 的值，将目标的精确 address 在内存中的存放位置赋给 x7。第五行执行后则 x7 获得了目标指令的 address（其实就是在内存 text 段的 address）。最后通过第六行跳转到对应指令。</p> </li> </ul> <h3 id="supporting-procedures">Supporting Procedures</h3> <p>本部分进一步讲解 jal, jalr, bne, beq 这类能搞跳转的指令。</p> <ul> <li> <p>指令的存储</p> <p>在将指令的存储前，首先讲内存的架构。</p> <p><img src="/assets/img/blog_post/co/cod-43.png" alt="cod-43"/></p> <p>从下到上依次为：</p> <ol> <li>Reserved 此部分保留。</li> <li>Text 此部分 <strong>用于存储指令</strong>。之前看到指令的跳转也是以 byte 为单位的，这是因为它在内存之中，而内存的寻址亦为 byte 单位。</li> <li>Static Data 静态数据。这部分很好理解。</li> <li>Dynamic Data 动态数据。这部分很好理解。</li> <li>Stack 堆栈。这部分后面会讲，通常用于暂存一些 reg 中需要用到的量。</li> </ol> </li> <li> <p>常见的 Procedure Call 流程</p> <p>首先是 <strong>Caller</strong>。其指令一般为 <code class="language-plaintext highlighter-rouge">jal x1, ProcedureAddress</code>。注意此语句后， x1 中会 <strong>自动存放下一段指令的地址（即 PC + 4）</strong>。</p> <p>然后是 <strong>Callee</strong>。其指令一般为 <code class="language-plaintext highlighter-rouge">jalr x0, 0(x1)</code>。可以看到我们直接跳到 <code class="language-plaintext highlighter-rouge">(x1)</code>，因为在跳之前 <code class="language-plaintext highlighter-rouge">x1</code> 已被赋好了值；此外第一个 operand 为 <code class="language-plaintext highlighter-rouge">x0</code> 是因为我们不关心 callee 的地址，只用返回去即可（即没有必要存储 callee 的地址）。</p> </li> <li> <p>堆栈</p> <p>在指令跳转的途中，有可能我们后面希望跳回来，但同时也希望跳之前 Caller 使用的部分 reg 得以保留。通常这是很难做到的，所以我们使用堆栈来先将 Caller 使用的寄存器的值保存下来，Callee 使用寄存器后再复原，以便接下来 Caller 再使用。</p> <p>首先再次明确 RISC-V 寄存器的变量区域划分，同时我们也重新做一些记号：</p> <table> <thead> <tr> <th>Category</th> <th>Notation</th> <th>Register</th> </tr> </thead> <tbody> <tr> <td>Arguments</td> <td>a0 ~ a7</td> <td>x10 ~ x17</td> </tr> <tr> <td>Saved</td> <td>s0 ~ 11</td> <td>x8 ~ x9, x18 ~ x27</td> </tr> <tr> <td>Temporary</td> <td>t0 ~ t6</td> <td>x5 ~ x7, x28 ~ x31</td> </tr> </tbody> </table> <p>可以这样理解各类变量：比如在调用一个递归的 C 函数 <code class="language-plaintext highlighter-rouge">f(k)</code> 时，$k$ 和 $f(k)$ 的返回值都可以视作 Argument；而其中运算的中间值，递归完回来还要用的可视作 Saved；其中运算的中间值，递归完回来不用，仅仅作为中间运算结果的，可以视作 Temporary。</p> <p>之前讲到堆栈是地址最高的。同时，堆栈的 top 也是地址更低的。如下图所示。</p> <p><img src="/assets/img/blog_post/co/cod-44.png" alt="cod-44"/></p> <p>考虑这样一个例子：C 代码实现递归阶乘</p> <pre><code class="language-C">  int fact ( int n ) {
      if ( n &lt; 1 ) return ( 1 ) ;
      else return ( n * fact ( n - 1 ) ) ;
  } 
</code></pre> <p>其 RISC-V 汇编代码如下：</p> <p><img src="/assets/img/blog_post/co/cod-45.png" alt="cod-45"/></p> <p>Line 1-3 开了两个栈空间，并把 <code class="language-plaintext highlighter-rouge">ra</code> 和 <code class="language-plaintext highlighter-rouge">a0</code>（对应 x10，一般用于存储输入参数和输出结果）存入堆栈。</p> <p>Line 4-5 在比较 $n$ 和 $1$ 的关系。</p> <ul> <li> <p>如果 $ n \leq 1$，进入 <code class="language-plaintext highlighter-rouge">L1</code> (Line 9)。注意到 Line 9 首先将 <code class="language-plaintext highlighter-rouge">a0</code> 减了一以便开始下一层递归（这也是为什么开始要把 <code class="language-plaintext highlighter-rouge">a0</code> 存进堆栈）；Line 10 则跳转到 <code class="language-plaintext highlighter-rouge">fact</code> (Line 1)，此行指令也会把 <code class="language-plaintext highlighter-rouge">ra</code> 的值改变（这也是为什么开始要把 <code class="language-plaintext highlighter-rouge">ra</code> 存进堆栈）。</p> </li> <li> <p>执行完 $fact(n-1)$ 后，回到 Line 11。此时 <code class="language-plaintext highlighter-rouge">a0</code> 已然是递归后 $fact(n-1)$ 的结果，先把它转入 <code class="language-plaintext highlighter-rouge">t1</code> 临时变量中。随后读取之前存好的 <code class="language-plaintext highlighter-rouge">a0</code> 和 <code class="language-plaintext highlighter-rouge">ra</code>，读取之后 <code class="language-plaintext highlighter-rouge">a0</code> 就是当前进程的 argument（即 $n$），<code class="language-plaintext highlighter-rouge">ra</code> 就是要回去的 Caller 地址。</p> <p>Line 15 使得 <code class="language-plaintext highlighter-rouge">a0</code> 变身 result 为一会儿返回的 caller 服务。Line 16 跳回 Caller。</p> </li> </ul> <p><strong>SUMMARY</strong></p> <p>一般而言，caller 要对 temporary / argument 寄存器负责。即根据自己的情况决定这两类数据是否要存储到堆栈中。</p> <p>而 callee 则对 saved 寄存器和 ra 负责。按照约定，在使用 saved 寄存器前，callee 必须先将 saved 中的变量存入堆栈；此外，callee 在执行指令前必须先将 ra 存入寄存器来确保待会儿能重返 caller。</p> </li> </ul> <h3 id="other-width">Other Width</h3> <p>之前说的 load / store 都是以 dword 为单位的（指令为 <code class="language-plaintext highlighter-rouge">ld</code> 和 <code class="language-plaintext highlighter-rouge">sd</code>）。实际上也可以读写 byte / halfword / word。例如，要 load byte / halfword / word 的格式则为</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lb rd, offset(rs1)
lh rd, offset(rs1)
lw rd, offset(rs1)
</code></pre></div></div> <h3 id="ml-2-addressing-for-32-bit-immediate-and-addresses">ML 2: Addressing for 32-Bit Immediate and Addresses</h3> <p>这部分讲解 32 位立即数赋值 / 32 位寻址的机器码层面具体实现。</p> <ul> <li> <p>32-Bit Immediate Addressing (U-Type)</p> <p>指令 <code class="language-plaintext highlighter-rouge">lui</code>：将一个 register 的高 20 位设为常值。</p> <table> <thead> <tr> <th>Category</th> <th>Instruction</th> <th>Example</th> <th>Meaning</th> <th>Type</th> </tr> </thead> <tbody> <tr> <td>Data transfer</td> <td>Load upper immediate</td> <td><code class="language-plaintext highlighter-rouge">lui x5, 0x12345</code></td> <td>x5 = 0x12345000, Loads 20-bits constant shifted</td> <td>U</td> </tr> </tbody> </table> <p><img src="/assets/img/blog_post/co/cod-46.png" alt="cod-46"/></p> <p>实际上的 RISC-V reg 是 64 位的。在这里我们对低 32 位赋予立即数。之所以要分两次赋予常值是因为一次的大小不够。</p> </li> <li> <p>Branch Addressing (SB-Type)</p> <p>上回说到 bne 的格式是 <code class="language-plaintext highlighter-rouge">bne rs1, rs2, L1</code> 其中 <code class="language-plaintext highlighter-rouge">L1</code> 是某个标签，表示了某处的指令。然而实际上机器肯定无法识别 <code class="language-plaintext highlighter-rouge">L1</code>，所以实际上这里 L1 翻译成机器码是一个 offset。</p> <p>举例：<code class="language-plaintext highlighter-rouge">bne x10, x11, 2000</code>（2000 = 0111 1101 0000）</p> <p><img src="/assets/img/blog_post/co/cod-47.png" alt="cod-47"/></p> <p>上图的 Inst 那一行有些神金，首先一般不用 Inst 表示（就用 imm），然后应该最低位从 0 开始。</p> <p>注意，由于所有的 RISC-V 指令都是 32 位的（本课程不讨论 16 位），所以指令的地址（byte 单位）一定满足末两位为 0。所以 RISC-V 的设计者决定在实际机器编码时，省略 offset 的最后一位，只将 offset 中的非 LSB 部分写入机器码（这样可以使 offset 支持更远的距离）。</p> </li> <li> <p>Jump Addressing (UJ-Type)</p> <p>考虑 <code class="language-plaintext highlighter-rouge">jal</code> 指令，其指令格式为 UJ 型。</p> <p>举例：<code class="language-plaintext highlighter-rouge">jal x0 2000</code>，表示跳转到 $\text{offset} = 2000_{(10)}$ 的位置，没有向 <code class="language-plaintext highlighter-rouge">ra</code> 中存入内容。</p> <p><img src="/assets/img/blog_post/co/cod-48.png" alt="cod-48"/></p> <p>其中 2000 (offset) 存储在 imm 部分；x0 存储在 rd 部分，表示的是某个寄存器地址。</p> <p>注意其中 <code class="language-plaintext highlighter-rouge">imm[20] (inst[31])</code> 表示的是 <strong>符号位</strong>。当其为 $1$ 实际上是往之前的指令跳。</p> <p>此外，和 Branch Addressing 一样，offset 的最后一位不再反映到机器码中，默认 offset 的最后一位一定为零。</p> </li> <li> <p>An Example: C $\to$ Assembly $\to$ Machine</p> <p><img src="/assets/img/blog_post/co/cod-49.png" alt="cod-49"/></p> <p>解读 bne 那一行：rs1 表示的是 x9，rs2 表示的是 x24。对于 SB-Type 指令，funct7 和 rd 列共同组成了 immediate。可以看到本指令的立即数为 <code class="language-plaintext highlighter-rouge">000000000110(0)</code> （最后一个 0 在机器码中被省去），对应的就是 +12。而我们看到 bne 和 Exit 差 3 条指令，即为 12 Bytes。</p> </li> </ul> <h3 id="summary">SUMMARY</h3> <p><img src="/assets/img/blog_post/co/cod-36.png" alt="cod-36"/></p> <p>个人感觉指令基本上是按「格式」分类，而不是「作用」分类。</p> <ul> <li> <p>R 型：<strong>主要用于「算术操作」</strong>。rd 表示被赋值寄存器，rs1 和 rs2 表示操作数。</p> </li> <li> <p>I 型：<strong>目前可用于「load」「立即数算术」「jalr」</strong>。load 和 jalr 的共同点是都有一个基地址 rs1，同时有一个偏移量 imm。随后它们的 rd 都会改变，load 会把内存中读到的值放进 rd，jalr 会把 PC+4 放进 rd。</p> </li> <li> <p>SB 型：<strong>目前可用于「选择型跳转」</strong>。rs1 和 rs2 执行某种比较运算，跳转的偏移量存储在 imm 中。</p> </li> <li> <p>UJ 型：<strong>目前可用于「jal」</strong>。jal 只有一个偏移量 offset，同时也会把 PC+4 放进 rd。</p> </li> <li> <p>U 型：<strong>目前可用于「立即数赋值」</strong>。把 imm 放进 rd（取低 32 位）的高 20 位。</p> </li> <li> <p>S 型：<strong>目前可用于「store」</strong>。store 会把 rs2 寄存器中的内容写入基地址 rs1，偏移量 imm 的位置。</p> </li> </ul> <p>可以看到，rs1 / rs2 / rd / imm 都是有其意义的。</p> <ul> <li> <p>imm 通常表示常量，所以也经常表示 offset。注意 imm 的「上下界」（比如 <code class="language-plaintext highlighter-rouge">imm[31:12]</code>）有其意义。大概是，如果 imm 用于表示某指令的 X 部分，则 <code class="language-plaintext highlighter-rouge">imm[a:b]</code> 表示 imm 部分表示的是 <code class="language-plaintext highlighter-rouge">X[a:b]</code>。</p> </li> <li> <p>rs1 通常表示基地址。也有的时候表示第一个操作数。</p> </li> <li> <p>rd 通常是一个会被写入新内容的寄存器地址。</p> </li> <li> <p>rs2 则通常表示第二个操作数。</p> </li> </ul> <h3 id="some-extensions-of-ch2">Some Extensions of CH2</h3> <ol> <li> <p>Synchronization in RISC-V (RISC-V 同步问题)</p> <p>考虑同时有两个处理器 P1 和 P2 在读写同一块内存。如果不进行同步 (Synchronization) 则有可能产生冲突。</p> <p>首先介绍两个新指令 <code class="language-plaintext highlighter-rouge">lr.d</code>，<code class="language-plaintext highlighter-rouge">sc.d</code>：</p> <table> <thead> <tr> <th>Category</th> <th>Instruction</th> <th>Example</th> <th>Meaning</th> </tr> </thead> <tbody> <tr> <td>Data transfer</td> <td>Load-Reserved Dword</td> <td><code class="language-plaintext highlighter-rouge">lr.d rd, (rs1)</code></td> <td>从内存中地址为 rs1 加载 8 个字节，写入 rd，并对内存 <strong>双字注册保留</strong></td> </tr> <tr> <td>Data transfer</td> <td>Store-Conditional Dword</td> <td><code class="language-plaintext highlighter-rouge">sc.d rd, rs2, (rs1)</code></td> <td>若内存地址 rs1 上存在加载保留，将 rs2 寄存器中的 8 字节数存入该地址，并向寄存器 rd 中存入 0；否则存入非 0 错误码</td> </tr> </tbody> </table> <p>简单来说，<code class="language-plaintext highlighter-rouge">lr.d</code> 会对地址为 <code class="language-plaintext highlighter-rouge">rs1</code> 的内存打上标记。若在下次 <code class="language-plaintext highlighter-rouge">rs1</code> 被 <code class="language-plaintext highlighter-rouge">sc.d</code> 之前 <code class="language-plaintext highlighter-rouge">rs1</code> 没被动过，则 <code class="language-plaintext highlighter-rouge">sc.d</code> 成功执行并返回 $0$；反之（被动过）则 <code class="language-plaintext highlighter-rouge">sc.d</code> 返回 $1$。</p> <p>接下来介绍两个例子，通过 <code class="language-plaintext highlighter-rouge">lr.d</code> 和 <code class="language-plaintext highlighter-rouge">sc.d</code> 来避免内存的读写冲突。</p> <ul> <li> <p>Example 1. Atomic Swap</p> <div class="language-as highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nx">again</span><span class="o">:</span> <span class="nx">lr</span><span class="p">.</span><span class="nx">d</span> <span class="nx">x10</span><span class="p">,</span> <span class="p">(</span><span class="nx">x20</span><span class="p">)</span>
  <span class="nx">sc</span><span class="p">.</span><span class="nx">d</span> <span class="nx">x11</span><span class="p">,</span> <span class="p">(</span><span class="nx">x20</span><span class="p">),</span> <span class="nx">x23</span>  <span class="c1">// X11 = status</span>
  <span class="nx">bne</span> <span class="nx">x11</span><span class="p">,</span> <span class="nx">x0</span><span class="p">,</span> <span class="nx">again</span>    <span class="c1">// branch if store failed</span>
  <span class="nx">addi</span> <span class="nx">x23</span><span class="p">,</span> <span class="nx">x10</span><span class="p">,</span> <span class="mi">0</span>      <span class="c1">// X23 = loaded value</span>
</code></pre></div> </div> <p>本段 RISC-V 指令实现了将寄存器 x23 和地址为 x20 的内存进行数值交换。注意如果 x11 非零说明 <code class="language-plaintext highlighter-rouge">sc.d</code> 不成功，即一二行指令之间可能发生了 x20 又被其他处理器覆写的情况。这种情况我们重新到 again 开始 swap，保证所有操作是 atomic 的。</p> </li> <li> <p>Example 2. Lock</p> <div class="language-as highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nx">addi</span> <span class="nx">x12</span><span class="p">,</span> <span class="nx">x0</span><span class="p">,</span> <span class="mi">1</span>         <span class="c1">// copy locked value</span>
  <span class="nl">again</span><span class="p">:</span> <span class="nx">lr</span><span class="p">.</span><span class="nx">d</span> <span class="nx">x10</span><span class="p">,</span> <span class="p">(</span><span class="nx">x20</span><span class="p">)</span>  <span class="c1">// read lock</span>
  <span class="nx">bne</span> <span class="nx">x10</span><span class="p">,</span> <span class="nx">x0</span><span class="p">,</span> <span class="nx">again</span>      <span class="c1">// check if it is 0 yet</span>
  <span class="nx">sc</span><span class="p">.</span><span class="nx">d</span> <span class="nx">x11</span><span class="p">,</span> <span class="p">(</span><span class="nx">x20</span><span class="p">),</span> <span class="nx">x12</span>    <span class="c1">// attempt to store</span>
  <span class="nx">bne</span> <span class="nx">x11</span><span class="p">,</span> <span class="nx">x0</span><span class="p">,</span> <span class="nx">again</span>      <span class="c1">// branch if fail</span>
</code></pre></div> </div> <p>To unlock:</p> <div class="language-as highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="nx">sd</span> <span class="nx">x0</span><span class="p">,</span> <span class="mi">0</span><span class="p">(</span><span class="nx">x20</span><span class="p">)</span>           <span class="c1">// free lock</span>
</code></pre></div> </div> <p>x12 中始终为 1。若 x20 为 1（表示锁住状态）则会一直在二三行之间循环，也就是指令被锁在这里了。如果要解锁则将 x20 设为 0（表示解锁状态），此时能顺利通过第三行。</p> <p>通过第三行后，再次尝试对锁复位为 1。若复位成功，x11 为 0，通过第五行，离开本部分指令；如果复位失败，x11 不为 0，说明二四行之间可能发生了 x20 被其他处理器覆写的情况，说明锁还不能松（其它进程在进行），需要回到 again 处重新循环上锁。</p> </li> </ul> </li> <li> <p>Translating and starting a program (程序的编写和执行步骤)</p> <p><img src="/assets/img/blog_post/co/cod-50.png" alt="cod-50"/></p> <ul> <li> <p>关于 obj 文件</p> <p>object 文件由六部分构成：</p> <ul> <li><strong>Header</strong>: described contents of object module</li> <li><strong>Text segment</strong>: translated instructions</li> <li><strong>Static data segment</strong>: data allocated for the life of the program</li> <li><strong>Relocation info</strong>: for contents that depend on absolute location of loaded program</li> <li><strong>Symbol table</strong>: global definitions and external refs</li> <li><strong>Debug info</strong>: for associating with source code</li> </ul> <p>一个例子如下，其中具有 A / B 两个程序的 obj 文件：</p> <p><img src="/assets/img/blog_post/co/cod-51.png" alt="cod-51"/></p> <p>将其 Link 之后，生成 Executable 文件如下：</p> <p><img src="/assets/img/blog_post/co/cod-52.png" alt="cod-52"/></p> </li> <li> <p>关于 Dynamic Linking &amp; Lazy Linkage</p> <p><img src="/assets/img/blog_post/co/cod-53.png" alt="cod-53"/></p> <p>可以理解为，我在某个程序里调用了一个库函数 F，但实际上 F 的内容并没有被写入 Text 段，而是在其他位置。为了调用时跳转到该位置，我需要先在 Memory Data（记为 MD）中找到 DLL（动态链接库）的位置，然后跳过去；而后 DLL 会告诉我（为我分配）F 的准确位置，然后我再跳一次。是为 Dynamic Linking。</p> <p>第二次调用库函数 F 时，MD 就已经存放了我要的 F 的位置了，直接就可以跳过去。是为 Lazy Linkage。</p> </li> </ul> </li> <li> <p>A C Sort Example To Put it All Together</p> <p>见讲义。一个主要思想是，把 C 转化成汇编语言的范式：</p> <p>Step 1. Allocate registers to program variables</p> <p>Step 2. Produce code for the body of the procedures</p> <p>Step 3. Preserve registers across the procedures invocation</p> </li> </ol>]]></content><author><name></name></author><category term="Course"/><category term="Notes"/><category term="Hardware"/><summary type="html"><![CDATA[Contains content about ISA.]]></summary></entry><entry><title type="html">Computer Organization Note (Part 3 of 4)</title><link href="https://sqrtyz.github.io/blog/2024/CO-NOTE-3/" rel="alternate" type="text/html" title="Computer Organization Note (Part 3 of 4)"/><published>2024-01-01T12:00:00+00:00</published><updated>2024-01-01T12:00:00+00:00</updated><id>https://sqrtyz.github.io/blog/2024/CO-NOTE-3</id><content type="html" xml:base="https://sqrtyz.github.io/blog/2024/CO-NOTE-3/"><![CDATA[<h2 id="chapter-4-1-processor-single-cycle-ver">Chapter 4-1 Processor: Single Cycle Ver</h2> <h3 id="introduction">Introduction</h3> <p>基本的思路是，在之前我们学习了 C -&gt; Assembly Language -&gt; Machine Code 的过程。而 CPU 的工作就是执行机器码。例如，对于指令 <code class="language-plaintext highlighter-rouge">add x10, x11, x12</code>，我们要用 CPU 完成以下工作：</p> <ul> <li> <p>读取 <code class="language-plaintext highlighter-rouge">x11</code> 和 <code class="language-plaintext highlighter-rouge">x12</code></p> </li> <li> <p>计算两者的和</p> </li> <li> <p>把和写到 <code class="language-plaintext highlighter-rouge">x10</code> 中</p> </li> </ul> <p>在本门课程中，我们主要关心以下指令：</p> <table> <thead> <tr> <th>Inst Type</th> <th>Inst</th> </tr> </thead> <tbody> <tr> <td>R-Type</td> <td>Arithmetic (<code class="language-plaintext highlighter-rouge">add</code>, <code class="language-plaintext highlighter-rouge">sub</code>, <code class="language-plaintext highlighter-rouge">and</code>, …)</td> </tr> <tr> <td>I-Type</td> <td>Load Register (<code class="language-plaintext highlighter-rouge">ld</code>)</td> </tr> <tr> <td>S-Type</td> <td>Store Register (<code class="language-plaintext highlighter-rouge">sd</code>)</td> </tr> <tr> <td>SB-Type</td> <td>Branch Jump (<code class="language-plaintext highlighter-rouge">beq</code>)</td> </tr> <tr> <td>UJ-Type</td> <td>Unconditional Jump (<code class="language-plaintext highlighter-rouge">jal</code>)</td> </tr> </tbody> </table> <h3 id="the-entire-processor">The Entire Processor</h3> <p><img src="/assets/img/blog_post/co/cod-54.png" alt="cod-54"/></p> <p>其实整个一节的关键信息就这一幅图。其中的一些关键点：</p> <ul> <li> <p>PC 表示当前的指令位置。</p> </li> <li> <p>Instruction Memory 存放的是系列指令，可以理解为内存中的 Text 段。</p> </li> <li> <p>Registers 寄存器。RISC-V 具有 32 个各 64 位的寄存器。</p> </li> <li> <p>Data Memory 存放的是各种数据，可理解为内存中的 Data 段。</p> </li> </ul> <p>黑色的部分对应 CPU 的 Data Path，蓝色的部分对应 CPU 的 Control Unit。可以看到，Control 部分由 Opcode 全权完成。</p> <p>接下来根据指令类型的不同，详细分析数据的走向。注意下面的图片仅作示意，不一定严谨。有的图片还有订正。</p> <h3 id="processor-datapath-details">Processor Datapath Details</h3> <h4 id="1-r-type">1. R-Type</h4> <p><img src="/assets/img/blog_post/co/cod-55.png" alt="cod-55"/></p> <h4 id="2-i-type-only-ld">2. I-Type (ONLY ld)</h4> <p><img src="/assets/img/blog_post/co/cod-58.png" alt="cod-58"/></p> <p><strong>[ATTENTION]</strong> 图片有点问题，指令中的 <code class="language-plaintext highlighter-rouge">rs1</code> 连到寄存器的 <code class="language-plaintext highlighter-rouge">rs2</code> 上了，应当连到 <code class="language-plaintext highlighter-rouge">rs1</code> 上。</p> <ul> <li>其中 <code class="language-plaintext highlighter-rouge">20{inst[31]}</code> 是在做符号位拓展，因为本来也支持立即数为负的情况。</li> </ul> <h4 id="3-s-type-only-sd">3. S-Type (ONLY sd)</h4> <p><img src="/assets/img/blog_post/co/cod-57.png" alt="cod-57"/></p> <h4 id="4-sb-type-only-beq">4. SB-Type (ONLY beq)</h4> <p><img src="/assets/img/blog_post/co/cod-56.png" alt="cod-56"/></p> <ul> <li>这里 ALU 执行的是 sub 运算，然后根据 <code class="language-plaintext highlighter-rouge">zero</code> 判断是否相等。如果 <code class="language-plaintext highlighter-rouge">zero</code> 为 1（beq 条件满足）且 <code class="language-plaintext highlighter-rouge">branch</code> 为 1（本条指令为 beq），那么就开跳。</li> </ul> <h4 id="5-uj-type-only-jal">5. UJ-Type (ONLY jal)</h4> <p><img src="/assets/img/blog_post/co/cod-59.png" alt="cod-59"/></p> <ul> <li>注意是 PC+4 进入到 <code class="language-plaintext highlighter-rouge">Write Data</code> 中。</li> </ul> <h3 id="controller">Controller</h3> <p>回顾一开始整个单周期 CPU 的图片，我们再贴一次：</p> <p><img src="/assets/img/blog_post/co/cod-54.png" alt="cod-54"/></p> <p>根据 Opcode，我们需要通过 Controller 给出以下 8 个具体信号：</p> <ul> <li>R/W - RegWrite (0/1): 控制是否向寄存器里面写入。</li> <li>R/W - MemRead (0/1): 控制是否从内存读取。</li> <li>R/W - MemWrite (0/1): 控制是否向内存中写入数据。</li> <li>MUX - ALUSrc (0/1): 控制 ALU 下侧的数据来源，0 表示 rs2，1 表示 ImmGen。</li> <li>MUX - Branch (0/1): 表示当前指令是否为 branch jumping，用于控制下一个 PC。0 表示不为 branch，1 表示为 branch。结合 Zero 进行食用。</li> <li>MUX - Jump (0/1): 控制 PC 是否进行无条件跳转。例如当前指令为 <code class="language-plaintext highlighter-rouge">jal</code> 时该值为 1。</li> <li>MUX - MemtoReg (2 bits): 控制返回到 write address 中的数据来源。分别表示 PC+4，内存读出数据，ALU 计算结果。</li> <li>ALU op (2 bits): 用于指导 ALU 进行的运算类型。后续会过一次 ALU Control。</li> </ul> <p><strong>接下来专门讲一下 ALU 的具体控制</strong>。可以看到，ALU 的控制可以视为「两级」：</p> <ul> <li> <p>第一级：根据 ALU op 将不同类别的指令进行基础区分。</p> </li> <li> <p>第二级：相同 ALU op 的基础上，再根据 funct3 和 funct7 对不同运算的指令进行区分（这部分 funct 从上图的 <code class="language-plaintext highlighter-rouge">inst[30,14:12]</code> 线钻进去）。</p> </li> </ul> <p><img src="/assets/img/blog_post/co/cod-60.png" alt="cod-60"/></p> <p>或许可以回顾一下 ALU：</p> <p><img src="/assets/img/blog_post/co/cod-9.png" alt="cod-9"/></p> <h2 id="chapter-4-2-processor-pipeline-ver">Chapter 4-2 Processor: Pipeline Ver</h2> <h3 id="intro">Intro</h3> <ul> <li> <p>不同指令耗时不一</p> <p>例如在下图中，假设各部分的操作时长为：Memory Access, 200ps; ALU, 200ps; RegFile Access, 100ps。</p> <p>那么执行一次 <code class="language-plaintext highlighter-rouge">ld</code> 指令耗时为 800ps。执行一次 <code class="language-plaintext highlighter-rouge">sd</code> 指令耗时为 700ps。</p> <p><img src="/assets/img/blog_post/co/cod-61.png" alt="cod-61" width="80%"/></p> </li> <li> <p>Pipelining Analogy</p> <p>一种优化思路是从瓶颈（例如 <code class="language-plaintext highlighter-rouge">ld</code>）下手。然而我们还有另外一种优化思路——将【单周期 CPU】改为【流水线 CPU】。所谓流水线，可以参照这幅图：</p> <p><img src="/assets/img/blog_post/co/cod-62.png" alt="cod-62" width="70%"/></p> <p>对应到 CPU 中，我们可以尝试把其分为若干个区域（或步骤），让每个区域同时工作，实现整体的流水线工作。</p> </li> </ul> <h3 id="risc-v-pipeline-intro">RISC-V Pipeline Intro</h3> <p>RISC-V CPU 可以划分为以下五个阶段：</p> <ol> <li><strong>IF</strong>: Instruction fetch from memory</li> <li><strong>ID</strong>: Instruction decode &amp; read from register</li> <li><strong>EX</strong>: Execute operation or calculate address (ALU Part)</li> <li><strong>MEM</strong>: Access memory operand</li> <li><strong>WB</strong>: Write result back to register</li> </ol> <p>然后，一种简单的思路就是，我让五个阶段「流水线式」地工作，下图地两张图生动地说明了这一原理：</p> <p><img src="/assets/img/blog_post/co/cod-63.png" alt="cod-63" width="90%"/></p> <p><img src="/assets/img/blog_post/co/cod-64.png" alt="cod-64" width="90%"/></p> <p>理想情况下，这一转变将让 CPU 提速 5 倍（因为有 5 个阶段，当然实际不会到达这个数值，看上图的下半部分可以理解；而且后面还有各种 hazards）。</p> <h3 id="hazards">Hazards</h3> <p>中文意为「冒险」，表示流水线 CPU 实现中可能遇到的各种容易出问题的情况。</p> <h4 id="1-structure-hazard">1. Structure Hazard</h4> <p>可以想象，如果 data memory 和 instruction memory 是在同一部分的话，在 pipeline 的情形下，有的周期会调用同一块内存，这将产生冲突。</p> <p>好在 RISC-V 的设计将 data memory 和 instruction memory 分隔开，所以我们几乎不用考虑 Structure Hazard。</p> <h4 id="2-data-hazard">2. Data Hazard</h4> <ul> <li> <p>Problem: Data Hazard</p> <p>有的时候执行某一个指令，要求前一个指令完成数据写入。考虑这样的两个相邻指令：</p> <pre><code class="language-ass">  add x19, x0, x1
  sub x2, x19, x3
</code></pre> <p><img src="/assets/img/blog_post/co/cod-65.png" alt="cod-65" width="90%"/></p> <p>add 指令的 WB 必须要在 sub 指令的 ID 之前执行（原因显然），所以两个指令之间无法再「紧密地排列在流水线上」，中间需要隔两个 bubble。</p> <p>【OBSERVATION】观察上上张图，WB 的写 reg 是在上半 cycle，ID 的读 reg 是在下半 cycle。这种设计的缘由其实在这里有所体现。</p> </li> <li> <p>Solution: Forwarding</p> <p>解决这种情况的一种方案是 Forwarding。大概思路是有的时候我们不必等待一个数据被写入 reg，然后才再次使用；而是可以直接用某种 extra connection 来直接高效地使用。下图演示了 Forwarding 的解决方案。</p> <p><img src="/assets/img/blog_post/co/cod-66.png" alt="cod-66" width="90%"/></p> <p>然而，并非所有 Data Hazard 都可以用 Forwarding 解决。考虑这样的情形：</p> <p><img src="/assets/img/blog_post/co/cod-67.png" alt="cod-67" width="90%"/></p> <p>显然 sub 已经没办法再提前一个 cycle 了，不然 sub 的 EX 和 ld 的 MEM 将处在同一 cycle，而 sub 的 EX 需要 ld 的 MEM 提供前提数据。</p> <p>不过，也可以通过修改汇编代码来规避这一问题——</p> <p><img src="/assets/img/blog_post/co/cod-68.png" alt="cod-68" width="90%"/></p> </li> </ul> <h4 id="3-control-hazard">3. Control Hazard</h4> <ul> <li> <p>Problem: Stall on Branch</p> <p><img src="/assets/img/blog_post/co/cod-69.png" alt="cod-69" width="80%"/></p> <p>类似于这种 Branch Jumping 带来的停顿。上图有点问题，按理来说应当是两行 bubble。</p> </li> <li> <p>Solution: Branch Prediction</p> <p>字面意思，提前预测 branch 的结果。一般可以分为静态预测 (Static branch prediction) 和动态预测 (Dynamic branch prediction)。</p> <p>静态预测一般是在软件层面上，根据普遍的 branch 选择结果进行预测。</p> <p>动态预测一般是在硬件层面上，比如可以基于上一次 branch 的选择结果进行预测（考虑循环的情形，这种预测应当是成功率较高的）。</p> </li> </ul> <h3 id="risc-v-pipelined-datapath">RISC-V Pipelined Datapath</h3> <p>在解决这一系列 hazards 之前，我们需要对 CPU 的结构进行修改。总的图形如下：</p> <p><img src="/assets/img/blog_post/co/cod-70.png" alt="cod-70" width="100%"/></p> <ul> <li> <p>四个中间寄存器：<code class="language-plaintext highlighter-rouge">IF/ID</code>，<code class="language-plaintext highlighter-rouge">ID/EX</code>，<code class="language-plaintext highlighter-rouge">EX/MEM</code> 以及 <code class="language-plaintext highlighter-rouge">MEM/WB</code>。</p> <p>字面意思表明了其所处位置。引入寄存器的目的是显而易见的，是为了使得在流水线上，每个周期的指令依然能正确读取数据。比如观察 Reg 的 Write Data 输入，肯定得引入 <code class="language-plaintext highlighter-rouge">MEM/WB</code> 的值，而不是当前 Instruction Memory 的值。它们之间差了有 3 个 CC。</p> <p>注意这个改变只是最基本的，保证 pipeline 能正常运作的。它还没有解决 hazards 的问题。</p> </li> <li> <p>Control 信号：<code class="language-plaintext highlighter-rouge">EX</code>，<code class="language-plaintext highlighter-rouge">MEM(M)</code> 以及 <code class="language-plaintext highlighter-rouge">WB</code>。</p> <p>回顾单周期 CPU，除去 <code class="language-plaintext highlighter-rouge">jal</code> 使用的 jump 信号，Controller 一共给出 7 个信号。根据其作用阶段的不同，将其分为</p> <ol> <li> <p>EX (作用于 EX 阶段)：包含 <code class="language-plaintext highlighter-rouge">ALUop</code>，<code class="language-plaintext highlighter-rouge">ALUsrc</code>。</p> </li> <li> <p>MEM (作用于 MEM 阶段)：包含 <code class="language-plaintext highlighter-rouge">Branch</code>，<code class="language-plaintext highlighter-rouge">MemRead</code>，<code class="language-plaintext highlighter-rouge">MemWrite</code>。</p> </li> <li> <p>WB (作用于 WB 阶段)：包含 <code class="language-plaintext highlighter-rouge">MemToReg</code>，<code class="language-plaintext highlighter-rouge">RegWrite</code>。</p> </li> </ol> <p>可以看到，在流水线上 Controller 对应的寄存器的大小是越来越小的，这一点也很好理解，毕竟到后面的 stage 某些 control signal 就不再起作用了。</p> </li> </ul> <h3 id="data-hazard-solution">Data Hazard Solution</h3> <h4 id="1-r-r-case">1. R-R Case</h4> <p>情景：考虑以下的 RISC-V 指令：</p> <pre><code class="language-asm">sub x2, x1, x3
and x12, x2, x5
or x13, x6, x2
add x14, x2, x2
sd x15, 100(x2)
</code></pre> <p><img src="/assets/img/blog_post/co/cod-71.png" alt="cod-71" width="100%"/></p> <p>从上图可以看出，Data Hazard 发生当且仅当</p> <ul> <li> <p>前序指令的 <code class="language-plaintext highlighter-rouge">rd</code> 和后序指令的 <code class="language-plaintext highlighter-rouge">rs1/rs2</code> 重合，即</p> <p>1a. <code class="language-plaintext highlighter-rouge">EX/MEM.RegisterRd</code> = <code class="language-plaintext highlighter-rouge">ID/EX.RegisterRs1</code></p> <p>1b. <code class="language-plaintext highlighter-rouge">EX/MEM.RegisterRd</code> = <code class="language-plaintext highlighter-rouge">ID/EX.RegisterRs2</code></p> <p>2a. <code class="language-plaintext highlighter-rouge">MEM/WB.RegisterRd</code> = <code class="language-plaintext highlighter-rouge">ID/EX.RegisterRs1</code></p> <p>2b. <code class="language-plaintext highlighter-rouge">MEM/WB.RegisterRd</code> = <code class="language-plaintext highlighter-rouge">ID/EX.RegisterRs2</code></p> <p>四条中有一条满足即可。</p> </li> <li> <p>执行写入操作</p> <p><code class="language-plaintext highlighter-rouge">EX/MEM.Controller.RegWrite</code> $= 1$</p> <p><code class="language-plaintext highlighter-rouge">MEM/WB.Controller.RegWrite</code> $= 1$</p> <p>要求哪条满足主要是看上面选的是 1 还是 2。</p> </li> <li> <p>不是虚空写入</p> <p><code class="language-plaintext highlighter-rouge">EX/MEM.RegisterRd</code> $\neq 0$</p> <p><code class="language-plaintext highlighter-rouge">MEM/WB.RegisterRd</code> $\neq 0$</p> <p>要求哪条满足主要是看上面选的是 1 还是 2。</p> </li> </ul> <p>所以，我们可以加入这样的逻辑来完成 Forwarding：</p> <p><img src="/assets/img/blog_post/co/cod-72.png" alt="cod-72" width="100%"/></p> <h4 id="2-load-r-case">2. Load-R Case</h4> <p>情景：考虑以下的 RISC-V 指令：</p> <pre><code class="language-asm">ld x2, 20(x1)
and x4, x2, x5
or x8, x2, x6
add x9, x4, x2
</code></pre> <p><img src="/assets/img/blog_post/co/cod-73.png" alt="cod-73" width="100%"/></p> <p>可以发现，第二行的指令无论如何都没有机会执行了，所以 <code class="language-plaintext highlighter-rouge">add x4, x2, x5</code> 必然会推后一个 CC。对于这一情况的判断，我们可以在上图橙色处完成。判断条件为</p> <ul> <li> <p>前序指令的 <code class="language-plaintext highlighter-rouge">rd</code> 和后序指令的 <code class="language-plaintext highlighter-rouge">rs1/rs2</code> 重合，即</p> <p>a. <code class="language-plaintext highlighter-rouge">ID/EX.RegisterRd = IF/ID.RegisterRs1</code></p> <p>b. <code class="language-plaintext highlighter-rouge">ID/EX.RegisterRd = IF/ID.RegisterRs2</code></p> <p>两条中有一条满足即可。注意上面条件的两侧不要搞反。</p> </li> <li> <p>执行 Load 操作</p> <p><code class="language-plaintext highlighter-rouge">ID/EX.Controller.MemRead</code> $= 1$</p> </li> </ul> <p>如果条件满足，我们首先要进行 Stall（熄火）。操作为</p> <ol> <li> <p>将当前指令（也就是下一个 CC 的 ID/EX）寄存器全部归零。</p> </li> <li> <p>再次读取 PC，即不调到 PC+4。</p> </li> </ol> <p>Stall 后，我们再执行 Forwarding 来保证指令的正常运行。这一部分的 Forwarding 操作和 R-R Case 几乎一致，唯一的区别是 Forwarding 的执行是在判断生效的两个 CC 后。</p> <p>下图展示了考虑 Load-R Case 的 Pipeline。可以看到主要区别是加入了 Stall 的考量。</p> <p><img src="/assets/img/blog_post/co/cod-74.png" alt="cod-74" width="100%"/></p> <h3 id="branch-hazard-solution">Branch Hazard Solution</h3> <h4 id="1-static-prediction">1. Static Prediction</h4> <p>考虑以下指令：</p> <pre><code class="language-asm">36: sub x10, x4, x8
40: beq x1, x3, 32 // PC-relative branch to 40+32=72
44: and x12, x2, x5
48: orr x13, x2, x6
52: add x14, x4, x2
56: sub x15, x6, x7
...
72: ld x4, 50(x7)
</code></pre> <p>我们假设采取静态预测（预测为 Not Taken），那么有</p> <p><img src="/assets/img/blog_post/co/cod-75.png" alt="cod-75" width="90%"/></p> <p>判断预测是否正确是看 <code class="language-plaintext highlighter-rouge">x1</code> 和 <code class="language-plaintext highlighter-rouge">x3</code> 是否相等。如果预测正确（的确为 Not Taken），则无事发生，所有指令照常进入流水线；如果预测错误（实际为 Taken），则需要</p> <ul> <li> <p>立刻对 IF/ID 寄存器进行 Flush，使得其下一个周期为 Bubble</p> </li> <li> <p>向 PC 中输入 PC+Offset，使 IF 获取跳转后的指令</p> </li> </ul> <p><img src="/assets/img/blog_post/co/cod-76.png" alt="cod-76" width="90%"/></p> <h4 id="2-dynamic-prediction">2. Dynamic Prediction</h4> <p>基本思路是，根据之前 branch 的选择结果决定之后的选择结果。这样做的依据是，假设我们处于一个循环体之中，大多数时候我们需要 take branch，只有循环开始 / 结束的时候才是 not take branch。</p> <ul> <li> <p>1-bit predictor</p> <p>拿一个一位的寄存器存储上一次的 branch 结果。下一次 predict 根据这个 1-bit predictor 做出预测。</p> </li> <li> <p>2-bit predictor</p> <p>只会在连续两次的错误预测之后才会更改预测。从直观上看这个 predictor 更为「固执」。</p> <p><img src="/assets/img/blog_post/co/cod-77.png" alt="cod-77" width="60%"/></p> </li> </ul> <h4 id="3-calculating-the-branch-target">3. Calculating the Branch Target</h4> <p>很明显，计算要跳到哪里 (Branch Target) 需要使用 ALU 进行计算，这会导致一定的延迟。</p> <p><strong>Branch Target Buffer</strong> 可以直接缓存下跳转的地址，下次就可以直接跳转并获取对应的指令。</p> <h3 id="exceptions-and-interrupts">Exceptions and Interrupts</h3> <ul> <li> <p>可能导致异常与中断的情况</p> <p>异常：例如未定义的 opcode, syscall 等。</p> <p>中断：例如外界 I/O 进行了操作导致指令中断。</p> </li> <li> <p>基本处理方法</p> <ol> <li>SEPC: 可以理解为存储出问题的 PC 的一个寄存器。</li> <li>SCAUSE: 可以理解为存储问题信息的一个寄存器。例如，SCAUSE 可以存储 opcode / hardware malfunction 等。</li> <li>handler: 可以理解为用于处理问题的专门指令。假设其地址位于 <code class="language-plaintext highlighter-rouge">0000 0000 1C09 0000</code>。</li> </ol> <p>出问题时，先将 PC 存储到 SEPC 中，然后跳到专门的 handler。</p> <p>handler 决定该采取何种措施。如果程序可重启，则采取适当措施后通过 SEPC 跳回去；反之则需要终止程序，根据 SEPC 和 SCAUSE 等信息报错。</p> </li> <li> <p>Pipeline with Exceptions</p> <p>可以看到，相比于处理 hazards 的 pipeline，这玩意儿主要是多了几个后面的指令寄存器的 flush。</p> <p><img src="/assets/img/blog_post/co/cod-78.png" alt="cod-78" width="90%"/></p> <p>具体地，例如 <code class="language-plaintext highlighter-rouge">add x1, x2, x1</code> 在 ALU 阶段发生异常，我们会执行以下步骤：</p> <ol> <li>保护 <code class="language-plaintext highlighter-rouge">x1</code> 不被异常地写入。</li> <li>完成 <code class="language-plaintext highlighter-rouge">add x1, x2, x1</code> 之前的指令。</li> <li>flush 掉 <code class="language-plaintext highlighter-rouge">add</code> 及之前的所有正在处理的指令（寄存器置零）。</li> <li>设置好 SEPC 和 SCAUSE。</li> <li>跳转到 handler。</li> </ol> </li> <li> <p>Multiple Exceptions</p> <p>多重异常的情况，一般是处理最先出现的异常。</p> </li> </ul> <h3 id="instruction-level-parallelism-ilp">Instruction-Level Parallelism (ILP)</h3> <p>之前提到的 CPU 加速的方法是流水线，此外还有一种可能是：指令并行 (ILP)。</p> <p>可以预见，这是一种非常冒险的举动，但它的确可以为 CPU 再提一下速度。</p> <h4 id="1-introduction-and-terms">1. Introduction and Terms</h4> <ul> <li> <p>Multiple Issue (多发)</p> <p>可分为 static multiple issue 和 dynamic multiple issue。</p> <p>Static Multiple Issue: 编译器来决定把哪些指令「捆在一起」，以规避可能发生的 hazards。</p> <p>Dynamic Multiple Issue: CPU 来检测指令流并决定把那些指令「捆在一起」，编译器只起到辅助作用。显然这个难度更大。</p> </li> <li> <p>Speculation (前瞻执行)</p> <p>某些指令可以更早地执行（例如，提前预测并执行 branch outcome 处的指令）。当然，如果 speculation 是错误的，则需要 rollback。</p> <p>前瞻执行可以由编译器完成，亦可以由硬件完成。</p> <p>可以结合接下来的 RISC-V 静态双指令并行的例子理解 speculation。</p> </li> </ul> <h4 id="2-static-multiple-issue">2. Static Multiple Issue</h4> <p>显然，静态多发要求各个 issue slot 之内的指令互不依赖。当然，issue slot 之间的指令是可能产生依赖的。必要时，编译器可能向某个 issue slot 中加入 bubble。</p> <ul> <li> <p>RISC-V 静态双指令并行</p> <p>基本思路：一个 issue slot 装两个指令，其中一个只能为 ALU/branch，另外一个只能是 load/store。例如：</p> <p><img src="/assets/img/blog_post/co/cod-79.png" alt="cod-79" width="70%"/></p> <p>考虑双指令并行的流水线 CPU 如下图所示：</p> <p><img src="/assets/img/blog_post/co/cod-80.png" alt="cod-80" width="90%"/></p> <p>例如，我们可以看到 ALU 多了一个，并且多出来的那个不再接 MUX。这是因为 ld/sd 用的 ALU，其 ALUSource 已经固定。</p> <hr/> <p>当然，「打包」不能乱打。例如以下两条指令显然是不能位于同一个 issue slot 的：</p> <pre><code class="language-asm">  add x10, x0, x1
  ld x2, 0(x10)
</code></pre> <p>一个打包的正确示例如下：</p> <p><img src="/assets/img/blog_post/co/cod-81.png" alt="cod-81" width="70%"/></p> <p>注意：</p> <ol> <li><code class="language-plaintext highlighter-rouge">ld x31, 0(x20)</code> 和 <code class="language-plaintext highlighter-rouge">add x31, x31, x21</code> 中间还隔了一个 CC，这是因为 load-use hazard 中间必须要空一个 CC。</li> <li><code class="language-plaintext highlighter-rouge">sd x31, 0(x20)</code> 变为 <code class="language-plaintext highlighter-rouge">sd x31, 8(x20)</code>，这是因为我们把 <code class="language-plaintext highlighter-rouge">addi</code> 移到了 <code class="language-plaintext highlighter-rouge">sd</code> 之前。所以后做 <code class="language-plaintext highlighter-rouge">sd</code> 的话就需要做一些调整。</li> <li><code class="language-plaintext highlighter-rouge">add x31, x31, x21</code> 和 <code class="language-plaintext highlighter-rouge">sd x31, 8(x20)</code> 之间的 data hazard 处理方法和之前讲的 R-R 基本一致，也是可以用 forwarding 无缝链接的。</li> </ol> </li> <li> <p>循环展开</p> <p>编译器层面 / 高级语言层面对循环进行展开，有利于指令并行加速。</p> <p>这个例子执行了对相邻内存中的数据增加一个固定值 <code class="language-plaintext highlighter-rouge">x21</code>。使用循环展开来使得 ALU/branch 和 load/store 能更好地耦合。</p> <p><img src="/assets/img/blog_post/co/cod-82.png" alt="cod-82" width="70%"/></p> </li> </ul> <h4 id="3-dynamic-multiple-issue">3. Dynamic Multiple Issue</h4> <ul> <li> <p>动态调整指令的顺序</p> <p>为了实现动态多发，不仅是要动态地把指令归到若干个 slots 中，必要时还可以动态调整指令的执行顺序。例如</p> <pre><code class="language-asm">  ld x31, 20(x21)
  add x1, x31, x2
  sub x23, x23, x3
  andi x5, x23, 20
</code></pre> <p>可以把 <code class="language-plaintext highlighter-rouge">add</code> 和 <code class="language-plaintext highlighter-rouge">sub</code> 对换，在不影响功能的基础上规避了一个 bubble。</p> </li> <li> <p>动态多发基本框架</p> <p><img src="/assets/img/blog_post/co/cod-83.png" alt="cod-83" width="80%"/></p> <p>乱序执行，顺序 commit。</p> </li> </ul>]]></content><author><name></name></author><category term="Course"/><category term="Notes"/><category term="Hardware"/><summary type="html"><![CDATA[Contains content about CPU design.]]></summary></entry><entry><title type="html">Computer Organization Note (Part 4 of 4)</title><link href="https://sqrtyz.github.io/blog/2024/CO-NOTE-4/" rel="alternate" type="text/html" title="Computer Organization Note (Part 4 of 4)"/><published>2024-01-01T12:00:00+00:00</published><updated>2024-01-01T12:00:00+00:00</updated><id>https://sqrtyz.github.io/blog/2024/CO-NOTE-4</id><content type="html" xml:base="https://sqrtyz.github.io/blog/2024/CO-NOTE-4/"><![CDATA[<h2 id="chapter-5-memory-hierarchy">Chapter 5 Memory Hierarchy</h2> <h3 id="introduction">Introduction</h3> <h4 id="transistor-晶体管">Transistor 晶体管</h4> <p><img src="/assets/img/blog_post/co/cod-84.png" alt="cod-84" width="80%"/></p> <p>类似于一个开关。栅极为 1 时两侧的 N 型半导体硅连通，input 连接到 output。</p> <h4 id="basic-memory-types">Basic Memory Types</h4> <ul> <li> <p>SRAM (Static Random Access Memory)</p> <p>六个晶体管。static 是指这种存储器只需要保持通电，里面的数据就可以永远保持。但是当断点之后，里面的数据仍然会丢失。</p> <p>SRAM 速度较快，但是成本更高，占用的空间更多。所以像诸如 CPU 的高速缓存，才会采用 SRAM。</p> <p><img src="/assets/img/blog_post/co/cod-85.png" alt="cod-85" width="40%"/></p> </li> <li> <p>DRAM (Dynamic Random Access Memory)</p> <p>一个电容加一个晶体管。由于 DRAM 使用电容存储，所以必须隔一段时间 refresh 一次，否则因为不断的微小漏电可能导致数据丢失。</p> <p>DRAM 速度较慢，但是成本更低。所以可以用于作为 main memory。</p> <p><img src="/assets/img/blog_post/co/cod-86.png" alt="cod-86" width="40%"/></p> </li> <li> <p>Disk Storage</p> <p><img src="/assets/img/blog_post/co/cod-98.png" alt="cod-98" width="80%"/></p> <ul> <li> <p>Structure</p> <ul> <li> <p><strong>Platter</strong> 盘子。一个磁盘一般有 4-16 个盘子。</p> </li> <li> <p><strong>Track</strong> 轨道。对应图上不同半径的环。</p> </li> <li> <p><strong>Sector</strong> 扇区。每个 track 又被分割为若干 sectors。</p> </li> <li> <p><strong>Cylinder</strong> 所有 platter 同一半径的 track 集合。</p> </li> <li> <p><strong>Read-write head</strong> 读写头。</p> </li> </ul> </li> <li> <p>Average Read Time of Disks</p> <p>基本方法为考虑四部分：</p> <ol> <li>寻道时间 Seek time</li> <li>旋转等待时间 Rotational latency</li> <li>数据传输时间 Transfer time</li> <li>控制用时 Control time</li> </ol> <p>【例题】Given 512B sector, 15,000 rpm, 4ms average seek time, 100MB/s transfer rate, 0.2ms controller overhead. Calculate average read time.</p> <ol> <li><strong>4ms</strong> seek time</li> <li>$\frac{1}{2} \times \frac{15000}{60}$ = <strong>2ms</strong> rotational latency（1/2 表示期望）</li> <li>$\frac{512B}{100MB/s}$ = <strong>0.005ms</strong> transfer time</li> <li><strong>0.2ms</strong> controller delay</li> </ol> <p>Total: <strong>6.2ms</strong></p> </li> </ul> </li> <li> <p>Flash Storage</p> <p>中文为闪存。比硬盘快了 100~1000 倍。Flash 又分为 NOR flash 和 NAND flash。</p> <p>我们熟知的 SSD（固态硬盘）一般就使用闪存存储数据。</p> </li> </ul> <h4 id="memory-hierarchy">Memory Hierarchy</h4> <p>Hierarchies bases on memories of different speeds and size.</p> <p>金字塔越往上，memory 更 expensive / small / fast。</p> <p><img src="/assets/img/blog_post/co/cod-87.png" alt="cod-87" width="90%"/></p> <h3 id="cache-basics">Cache Basics</h3> <h4 id="cache-intro---direct-mapped-cache">Cache Intro - Direct Mapped Cache</h4> <ul> <li> <p>基本思路</p> <p><img src="/assets/img/blog_post/co/cod-88.png" alt="cod-88" width="80%"/></p> <p><strong>(Cache Block Address)</strong> = <strong>(Memory Block Address)</strong> mod (Number of blocks in the cache)</p> <p>注意，一个 block 中不是一个 byte。RISC-V 中我们默认一个 block 的 <strong>最小大小</strong> 为 word (4 byte)。</p> <p>为了得知一个 cache block 中究竟存的是哪一个 memory block 的数据（例如上图中，一个灰色块可以对应 8 个灰色块），cache block 还会存下「其所存储的数据」对应的 memory block 地址的「高若干位」。具体地，一个 DMC 的结构如下所示：</p> </li> <li> <p>DMC 的结构</p> <p><img src="/assets/img/blog_post/co/cod-89.png" alt="cod-89" width="60%"/></p> <ol> <li> <p>V 表示 Valid bit，表示是否启用。启用为 1，反之为 0。</p> </li> <li> <p>Tag 即为刚刚所说的，「其所存储的数据」对应的 memory block 地址的「高若干位」。例如，对于上图例 Memory 中的最左边的灰色块，它的数据在 Cache 中存储在 001 的 Data 段，Tag 段对应为 00。</p> </li> <li> <p>Data 存储的就是数据。如果一个 block 的大小为 1 word，那么 Data 段的长度就是 32 位。</p> </li> </ol> </li> <li> <p>DMC 的寻址</p> <p>考虑一个情景，CPU 希望从内存中抓一个地址（记得是按 Byte 寻址哦）。这个 64 位的地址将会被分为 TAG / Index / Byte offset。其中 {Tag, Index} 联称 Memory Block Address。</p> <p>注意，当一个 block 包含 1 word 时，byte offset 占 2 位；但有时候一个 block 也可以占更多的字（比如 4 word），此时 byte offset 会占更多位（比如 4 位）。</p> <p><img src="/assets/img/blog_post/co/cod-90.png" alt="cod-90" width="80%"/></p> </li> </ul> <h4 id="cache-topic-1---block-placement">Cache Topic 1 - Block Placement</h4> <p><img src="/assets/img/blog_post/co/cod-91.png" alt="cod-91" width="80%"/></p> <p>除了 Direct Mapped，还有两种常见的映射策略：</p> <ul> <li> <p>Fully Associative</p> <p>内存中的 block 可以对到 cache 中的任何 block。好处是提高了 cache 的利用率，但是寻址会很麻烦。</p> </li> <li> <p>Set Associative</p> <p>介于 Direct Mapped 和 Fully Associative 之间。内存中的 block 可以对到 cache 中 <strong>某个 set</strong> 的任何 block。set 的选取和 block 一致，即 <strong>(Cache Set Address)</strong> = <strong>(Memory Block Address)</strong> mod <strong>(Number of sets in the cache)</strong></p> <p>对于 Set Associative，一个 set 如果包含 $k$ blocks，那么这个 cache 又被称为 $k$-way set associative。DM 和 FA 可以视作特殊的 SA。比如 DM 就是 $1$-way SA，FA 就是 $m$-way SA（其中 $m$ 表示 block 数目）。</p> </li> </ul> <h4 id="cache-topic-2---block-identification">Cache Topic 2 - Block Identification</h4> <p>基本思路就是按照 Direct Mapped 那样。</p> <p><img src="/assets/img/blog_post/co/cod-92.png" alt="cod-92" width="50%"/></p> <p>Direct Mapped 的具体寻址方式之前已经介绍。</p> <p>对于 Fully Associative，寻址方式如下：</p> <p><img src="/assets/img/blog_post/co/cod-93.png" alt="cod-93" width="70%"/></p> <p>对于 Set Associative，寻址方式如下：</p> <p><img src="/assets/img/blog_post/co/cod-94.png" alt="cod-94" width="80%"/></p> <h4 id="cache-topic-3---block-replacement">Cache Topic 3 - Block Replacement</h4> <p>如果一个 cache 满了，再从内存往里面放东西的话（例如 read miss），我们需要对其中的数据进行替代。</p> <ul> <li> <p>对于 Direct Mapped，没有这方面的担心。因为映射是多对一的，每次要么换要么不换。</p> </li> <li> <p>对于 Fully Associative 和 Set Associative，则需要考虑换掉哪个 block。一般有如下的策略：</p> <ol> <li> <p>Random replacement 字面意思，随机替换。</p> </li> <li> <p>Least-recently used (LRU) 最长时间没用的替换掉。</p> </li> <li> <p>First in, first out (FIFO) 队列思想，最早引入 cache 的替换掉。注意这和 LRU 不一样。</p> </li> </ol> </li> </ul> <h4 id="cache-topic-4---read-and-write-strategy">Cache Topic 4 - Read and Write Strategy</h4> <ul> <li> <p>Read Strategy</p> <ul> <li> <p>如果 Read hits，无事发生，这很好。</p> </li> <li> <p>如果 Read misses，我们需要执行一系列措施。实际上，Read misses 分为 instruction cache miss 和 data cache miss。以 instruction cache miss 为例：</p> <ol> <li> <p>将目前的 PC 值告诉 Memory，意思是我接下来要读 PC 对应的指令了，但是我在 Cache 部分没找到，所以要去 Memory 部分找。</p> </li> <li> <p>Memory access 到对应的 32 位指令。</p> </li> <li> <p>将对应的指令写进 instruction cache。其中 data 段放入指令本身，tag 段放入高位地址（ALU 来计算高位），valid bit 要设为 1。</p> </li> <li> <p>重启 PC 指令的 fetching，这次我们可以在 instruction cache 读取到它。</p> </li> </ol> </li> </ul> </li> <li> <p>Write Strategy</p> <ul> <li> <p>Write Hit Strategy</p> <ol> <li> <p>Write-back 只向缓存中写入数据，回头找个合适的时机写入内存。好处是很快，但是会导致数据的 inconsistent。</p> <p>此外，Write-back 不能直接丢弃 cached data（例如由 read miss 引起的替换），这是因为其中的值可能并未被同步到 memory。故而 cache 的 control bit 需要使用两位：valid bit【依然和原来一样，表示是否数据有效】和 dirty bit【表示是否同步到 memory 中，未同步则为 0】。如果替换时发现被替换者 dirty bit 为 0，则需要先将被替换者写入内存。</p> </li> <li> <p>Write-through 则同时向缓存和内存写入数据。好处是可以保证数据的 consistency，但是速度会较慢。</p> <p>对于 Write-through，我们可以放心地丢弃 cached data，因为其中的值和 memory 始终保持同步。此类 cache 的 control bit 只有 valid bit 一位。</p> </li> </ol> </li> <li> <p>Write Stall and Write Buffers</p> <p><strong>Write stall</strong> 指的是在 Write-through 中，CPU 必须等待 write to memory 完成所导致的 bubble。</p> <p><strong>Write buffer</strong> 如下图所示。对于 Write-through 的策略，由于理论上我们需要时刻把 cache 和 memory 保持同步，而这会耗费大量的时间。所以考虑 cache 同步到 memory 时，我们先把它写到一个 buffer 里面（这很快），写完后 CPU 继续干自己的活，buffer 则慢慢地把数据同步到 memory 中。</p> <p><img src="/assets/img/blog_post/co/cod-95.png" alt="cod-95" width="80%"/></p> </li> <li> <p>Write Miss Strategy</p> <ol> <li> <p>若 Write Hit Strategy 采取 Write-back，则 miss 时一般采取 Write allocate 策略（理论上也可以采取 Write around 策略）。</p> <p>Write allocate：<strong>如果 Write misses，先把对应的内存数据读到缓存中</strong>，转换为 Write hits 的情况再后续处理。</p> <p>注意，由于一个 block 可能包含若干个 word，所以我们也有必要这样做。考虑情景：本来一个 block 对应的 4 words 分别为 [A B C D]，考虑只写其中的一个 word。如果 write miss 发生，cache 中的对应 block 变为 [X E X X]（其中 [X X X X] 是对应 cache entry 的原始值），后面写到内存中，变为 [X E X X]，但实际上应该是 [A E C D]。</p> </li> <li> <p>若 Write Hit Strategy 采取 Write-through，则 miss 时一般采取 Write around 策略（理论上也可以采取 Write allocate 策略）。</p> <p>Write around：<strong>如果 Write misses</strong>，直接绕过 cache 把对应的数据写入 memory。想来确实也可以这么干。</p> </li> </ol> </li> </ul> </li> </ul> <h4 id="cache-memory-data-transfer">Cache-Memory Data Transfer</h4> <p>之前提到，如果发生 read miss 或者 write miss，则需要在 cache 与 memory 之间进行数据传输。对于不同的 Cache-Memory 结构，其传输效率也不一。</p> <p><img src="/assets/img/blog_post/co/cod-96.png" alt="cod-96" width="80%"/></p> <p>假设耗时（假设 Block size 为 4 words）：</p> <table> <thead> <tr> <th>Work</th> <th>CC Cost</th> </tr> </thead> <tbody> <tr> <td>send the address</td> <td>1</td> </tr> <tr> <td>DRAM access initiated</td> <td>15</td> </tr> <tr> <td>transfer a word of data</td> <td>1</td> </tr> <tr> <td>send the address</td> <td>1</td> </tr> </tbody> </table> <ol> <li> <p>One-word-wide memory organization</p> <p>最基本的架构。在以上假设下，读取一个 block 的耗时为 $1 + 4 \times (1+15)=65$ CC</p> </li> <li> <p>Wide memory organization</p> <p>更宽的总线和内存。在以上假设下，读取一个 block 的耗时为 $1 + (1+15)=17$ CC</p> <p>然而，随之而来的代价是硬件设计的要求提高（最直观的，占空间变多了）</p> </li> <li> <p>Interleaved memory organization</p> <p>内存分成几个 bank，这样一来内存部分可以同步进行 initialize。在以上假设下，读取一个 block 的耗时为 $1 + 4 \times 1 + 15=20$ CC</p> </li> </ol> <h3 id="measure-and-improve-cache-performance">Measure and Improve Cache Performance</h3> <h4 id="一些指标">一些指标</h4> <ul> <li> <p>Average Memory Assess Time (AMAT)</p> <p>= hit time + miss time</p> <p>= hit time + miss rate $\times$ miss penalty</p> </li> <li> <p>CPU Time (更新定义)</p> <p>= CPU execution clock cycles + Memory-stall clock cycles</p> </li> <li> <p>Memory-stall clock cycles</p> <p>= Number of instructions $\times$ miss rate $\times$ miss penalty</p> <p>(Also can be written as) = Read-stall cycles + Write-stall cycles</p> </li> <li> <p>Read-stall cycles</p> <p>= number of read instructions $\times$ read miss rate $\times$ read miss penalty</p> </li> <li> <p>Write-stall cycles <strong>(For Write-through strategy)</strong></p> <p>= number of write instructions $\times$ write miss rate $\times$ write miss penalty + write buffer stalls</p> </li> <li> <p>如果忽略 write buffer stalls，Read-stall cycles 和 Write-stall cycles 理论上可以合并。统一为</p> <p>Memory-stall clock cycles ＝ Memory access instructions $\times$ Miss rate $\times$ Miss penalty</p> </li> </ul> <h4 id="example-quiz">Example Quiz</h4> <p><img src="/assets/img/blog_post/co/cod-97.png" alt="cod-97" width="60%"/></p> <h4 id="miss-penalty-一图流">Miss Penalty 一图流</h4> <p><img src="/assets/img/blog_post/co/cod-99.png" alt="cod-99" width="80%"/></p> <h3 id="l1-and-l2-cache-hierarchy">L1 and L2 Cache Hierarchy</h3> <p>回顾之前的金字塔图，我们可以将 Cache 分为 L1 Cache 和 L2 Cache（两者均采用 SRAM，唯一的区别在于大小不同，L1 Cache 略快于 L2 Cache）。</p> <p>如果 L1 Cache miss 了，就去 L2 Cache 读取；如果再 miss，就去 DRAM。</p> <p>考虑这样一道计算题：</p> <ul> <li> <p>CPI of 1.0 on a 5GHz machine</p> </li> <li> <p>Initial: 2% miss rate, 100ns DRAM access</p> </li> <li> <p>Adding L2 Cache: 5ns access time, decreases miss rate to 0.5%</p> </li> </ul> <h3 id="virtual-memory">Virtual Memory</h3> <ul> <li> <p>Basic Concepts</p> <p>和之前印象中的 virtual memory 有点区别。似乎不是拿 disk 当 memory 用，据说 virtual memory 有很多所指。</p> <p>不管这些。我们接下来讨论的 virtual memory 实际上就是一个介于 CPU 和「实际地址」之间的「中介」。在实际过程中，CPU 只抛出虚拟地址，根据虚拟地址得到实际内存地址再去访问 cache memory (SRAM) / main memory (DRAM) / disk。</p> <p><img src="/assets/img/blog_post/co/cod-100.png" alt="cod-100" width="80%"/></p> </li> <li> <p>Fetching Physical Address Method I: Page Table</p> <p>一般来说 page table 位于 main memory 中，就像一般的内存那样存储了从 virtual address 到 physical address 的映射。</p> <p>如果某个数据不在 physical memory 中，则 virtual address 映射到的会是 disk address。此时称发生了 page fault，同时这会导致极大的 miss penalty。</p> <p><img src="/assets/img/blog_post/co/cod-101.png" alt="cod-101" width="70%"/></p> <p>下图展示了具体如何利用 page table 将 virtual address 映射到 physical address 的方法。前半部分拿进去查表，后半部分 offset 保持不变。当然，这只是 page table hit 的情况。</p> <p><img src="/assets/img/blog_post/co/cod-102.png" alt="cod-102" width="80%"/></p> </li> <li> <p>Fetching Physical Address Method II: TLB</p> <p>为了加速从 virtual address 找 physical address 这一过程，我们可以添加一个 TLB (Translation-lookaside Buffer) 模块。</p> <p>TLB 可以视作一个「关于 page table 的 fully-associative cache」。注意 TLB 只存储映射到 physical memory 的情形。</p> <p><img src="/assets/img/blog_post/co/cod-103.png" alt="cod-103" width="80%"/></p> </li> <li> <p>Whole Structure</p> <p>考虑 TLB 后，memory data 的获取流程如下（图中未显示 page table 的情形，不过应该比较好脑补）。</p> <p><img src="/assets/img/blog_post/co/cod-104.png" alt="cod-104" width="80%"/></p> <p><img src="/assets/img/blog_post/co/cod-105.png" alt="cod-105" width="80%"/></p> </li> <li> <p>理解检测</p> <p><img src="/assets/img/blog_post/co/cod-106.png" alt="cod-106" width="80%"/></p> </li> </ul> <h2 id="chapter-6-storage-networks-and-other-peripherals">Chapter 6 Storage, Networks and Other Peripherals</h2> <h3 id="introduction-1">Introduction</h3> <ul> <li> <p>Typical I/O Devices</p> <p><img src="/assets/img/blog_post/co/cod-107.png" alt="cod-107" width="80%"/></p> </li> <li> <p>Three Characters of I/O</p> <ol> <li> <p>Behavior</p> <p>行为。例如是做 input，还是 output，还是 storage。</p> </li> <li> <p>Partner</p> <p>对象。交互对象，例如 mouse 的 partner 是人，network 的 partner 是机器。</p> </li> <li> <p>Data Rate</p> <p>I/O device 和 main memory/processor 之间的数据传输速度峰值。</p> </li> </ol> </li> <li> <p>I/O Performance Measurement: <strong>Throughput</strong> and <strong>Response Time</strong></p> </li> </ul> <h3 id="disk-storage-and-dependability">Disk Storage and Dependability</h3> <ul> <li> <p>Availability Measurement</p> <ol> <li> <p>MTTF (Mean Time to Failure)</p> <p>平均故障时间。即一个部件期望的无故障运行时长。</p> </li> <li> <p>MTTR (Mean Time to Repair)</p> <p>平均修复时间。即一个部件发生故障后期望的修复所需时长。</p> </li> <li>MTBF (Mean Time Between Failures) = MTTF + MTTR</li> <li>Availability = MTTF / (MTTF + MTTR)</li> </ol> </li> <li> <p>Reliability Measurement</p> <p>考虑一个情景：我们能否使用非常多的小 disk 来组成大 disk，进而缩短 disk 和 CPU/Memory 之间的速度差距？</p> <p><img src="/assets/img/blog_post/co/cod-108.png" alt="cod-108" width="60%"/></p> <p>答案是否定的。因为 (MTTF of N disks) = (MTTF of 1 Disk) / N。如此设计会让 disk 的可靠性大幅下降。</p> <p>实际上 reliability 的衡量建立于 availability 的衡量之上：</p> <ol> <li> <p>AFR (annual failure rate) = percentage of devices to fail per year</p> <p>= (365 $\times$ 24) hours / MTTF in hours</p> </li> <li> <p>“nines of availability” per year (中间是 availability，右边是 meaning)</p> <p><img src="/assets/img/blog_post/co/cod-109.png" alt="cod-109" width="60%"/></p> </li> </ol> </li> <li> <p>Magnetic Disk</p> <p>详见 Chapter 5: Introduction: Basic Memory Type 部分。</p> </li> <li> <p>Flash Storage</p> <p>详见 Chapter 5: Introduction: Basic Memory Type 部分。</p> </li> <li> <p>RAID</p> <p><img src="/assets/img/blog_post/co/cod-110.png" alt="cod-110" width="60%"/></p> <p>DBS 部分已有介绍。这边只作简要补充。</p> <ul> <li> <p>RAID 0: No Redundancy (Skipped)</p> </li> <li> <p>RAID 1: Disk Mirroring/Shadowing (Skipped)</p> </li> <li> <p>RAID 2: Error Correction Code (Skipped, <em>UNUSED</em> now)</p> </li> <li> <p>RAID 3: Bit-Interleaved Parity Disk</p> <p><img src="/assets/img/blog_post/co/cod-113.png" alt="cod-113" width="50%"/></p> <p>其中 0, 1, 2, …, 23 是数据的实际顺序。但是每个方框只包含了一个 bit。</p> </li> <li> <p>RAID 4: Block-Interleaved Parity Disk</p> <p><img src="/assets/img/blog_post/co/cod-111.png" alt="cod-111" width="70%"/></p> <p>其中 0, 1, 2, …, 23 是数据的实际顺序。每个方框只包含了一个 block。</p> <p>【写入分析 1】一次 Logical Write 将会涉及到 2 次 Physical Read 和 2 次 Physical Write（如上右图所示）。</p> <p>【写入分析 2】对比 RAID 3 和 RAID 4 的 small writes（绿色部分示意，假设 8bit）。RAID 3 需要进行 4 次串行的「Main Disk + Parity Disk」的物理读取和写入（无法并行是因为 Parity Disk 在 4 次中都需要被访问）；而 RAID 4 由于 8bit 都位于 block 0 中，所以只需要一次「Main Disk + Parity Disk」的物理读取和写入。</p> </li> <li> <p>RAID 5: Block-Interleaved Distributed Parity Disk</p> <p><img src="/assets/img/blog_post/co/cod-112.png" alt="cod-112" width="80%"/></p> <p>其中 0, 1, 2, …, 23 是数据的实际顺序。</p> <p>【写入分析 3】这种分布式设计允许更为激进的并行写入。例如：我要 logical write D0 和 D5。对于 RAID 5，这 $2 \times 2$ 个 disks <strong>互不干扰</strong>，可以同时进行；但是对于 RAID 4，$P$ 校验位使用同一个 disk，logical write 必须分两波次进行。</p> </li> <li> <p>RAID 6: P + Q Redundancy</p> <p>在 RAID 5 的基础上，使用两个 Redundancy Disk。</p> </li> <li> <p>Comparison</p> <ol> <li>一般而言，RAID 3 比 RAID 4 更擅长长序列读取，RAID 4 比 RAID 3 更擅长小范围读取；</li> <li>RAID 3 在 small writes 上具有最低的 throughput；RAID 3. 4. 5 在 large writes 上具有几乎一致的 throughput。</li> </ol> </li> </ul> </li> </ul> <h3 id="buses">Buses</h3> <h4 id="buses-basics">Buses Basics</h4> <ul> <li> <p>Buses</p> <p>总线是多条线的组合，用于进行各模块之间的数据传输。</p> <p>分为 control lines 和 data lines，其中 data lines 可以传输地址和具体数据。</p> </li> <li> <p>Bus Transactions</p> <ol> <li> <p>Output 流程（CPU 指示内存向 devices 中写出数据）</p> <p><img src="/assets/img/blog_post/co/cod-114.png" alt="cod-114" width="80%"/></p> </li> <li> <p>Input 流程（CPU 指示内存从 devices 中读取数据）</p> <p><img src="/assets/img/blog_post/co/cod-115.png" alt="cod-115" width="70%"/></p> </li> </ol> </li> </ul> <h4 id="asynchronous-data-fetching-handshaking-protocol">Asynchronous Data Fetching: Handshaking Protocol</h4> <p>考虑一个情形：某个 I/O Device 希望从 memory 中读取数据。在「同步」和「异步」的情况下，读取方法有所差异。接下来介绍异步读取的「握手协议」：</p> <p><img src="/assets/img/blog_post/co/cod-116.png" alt="cod-116" width="90%"/></p> <p>橙色表示 I/O Device 的信号，黑色表示 memory 的信号。Ack 相当于一条辅助信号线，用于告诉对方「我收到了你的请求 / 回应」。</p> <ol> <li> <p>When memory sees the ReadReq line, it reads the address from the data bus, begin the memory read operation，then raises Ack to tell the device that the ReadReq signal has been seen.</p> </li> <li> <p>I/O device sees the Ack line high and releases the ReadReq data lines.</p> </li> <li> <p>Memory sees that ReadReq is low and drops the Ack line.</p> </li> <li> <p>When the memory has the data ready, it places the data on the data lines and raises DataRdy.</p> </li> <li> <p>The I/O device sees DataRdy, reads the data from the bus , and signals that it has the data by raising ACK.</p> </li> <li> <p>The memory sees Ack signals, drops DataRdy, and releases the data lines.</p> </li> <li> <p>Finally, the I/O device, seeing DataRdy go low, drops the ACK line, which indicates that the transmission is completed.</p> </li> </ol> <p>【例题】</p> <blockquote> <p>Assume: <strong>The synchronous bus</strong> has a clock cycle time of 50 ns, and each bus transmission takes 1 clock cycle. <strong>The asynchronous bus</strong> requires 40 ns per handshake. The data portion of both buses is 32 bits wide.</p> <p>Question: Find the <strong>bandwidth</strong> for each bus when reading one word from a 200-ns memory.</p> </blockquote> <ul> <li> <p>Synchronous Case</p> <ol> <li>Send the address to memory : 50ns</li> <li>Read the memory : 200ns</li> <li>Send the data to the device : 50ns</li> </ol> <p>Thus, the total time is 300 ns. So, the bandwidth = 4bytes/300ns = 13.3MB/s</p> </li> <li> <p>Asynchronous Case</p> <p>回顾之前的握手协议图。</p> <p>Step 1: 40ns</p> <p>Step 2, 3, 4: $\max$(2 $\times$ 40ns + 40ns, 200ns) = 200ns</p> <p>Step 5, 6, 7: 3 $\times$ 40ns = 120ns</p> <p>所以读取一个 word 的总时间为 $360\text{ns}$。转换成带宽即为 $11.1$ MB/s。</p> </li> </ul> <h4 id="bus-arbitration">Bus Arbitration</h4> <p>当总线被多个 I/O Device 申请使用时，需要有人完成调度。一般承担这个工作的是 processor。</p> <p>四个常见的调度模式：</p> <ul> <li>daisy chain</li> <li>centralized</li> <li>self selection</li> <li>collision detection</li> </ul> <h4 id="bus-bandwidth-computation">Bus Bandwidth Computation</h4> <ul> <li> <p>例题</p> <p><img src="/assets/img/blog_post/co/cod-117.png" alt="cod-117" width="80%"/></p> <p>Suppose we have a system with the following characteristic:</p> <ol> <li> <p>A memory and bus system supporting block access of (4~16) 32-bit words.</p> </li> <li> <p>A 64-bit synchronous bus clocked at 200 MHz, with each 64-bit transfer taking 1 clock cycle, and 1 clock cycle required to send an address to memory.</p> </li> <li> <p>Two clock cycles needed between each bus transaction.（每次利用总线读取一个 block 视作一个 transaction，每个 transaction 之间需要空 2 个周期）</p> </li> <li> <p>A memory access time for the first four words of 200ns; each additional set of four words can be read in 20 ns. Assume that a bus transfer of the most recently read data and a read of the next four words can be overlapped.</p> </li> </ol> <p>Q1. Find the <strong>sustained bandwidth</strong> and the latency for a read of 256 words for transfers that use <strong>4-word blocks</strong> and for transfers that use <strong>16-word blocks</strong>.</p> <p>Q2. Also compute <strong>effective number of bus transactions</strong> per second for each case.</p> </li> <li> <p>解答（4-word block case）</p> <p>For each block, it takes</p> <ol> <li><strong>1 CC</strong> to send the address to memory</li> <li>200ns/(5ns/cycle) = <strong>40 CC</strong> to read memory</li> <li><strong>2 CC</strong> to send the data from the memory</li> <li><strong>2 CC</strong> needed between each bus operation.</li> </ol> <p>This is a total of <strong>45 CC</strong>.</p> <p>Since there are 256/4 = 64 blocks, the transfer of 256 words takes 45 $\times$ 64 = 2880 CC.</p> <p>The latency for the transfer of 256 words is: 2880 cycles $\times$ (5ns/cycle) = 14,400 ns.</p> <p>Final answer:</p> <ul> <li> <p>Number of bus transactions per second:</p> <p>$64 \text{ transactions} \times \frac{1 \text{second}}{14,400 \text{ns}} = 4.44 \text{M transactions/s}$.</p> <p>注意，当使用 4-word block 进行传输时，一次 256-word transfer 实际上包含了 64 次 bus transactions。</p> </li> <li> <p>Bandwidth</p> <p>$1024 \text{bytes} \times \frac{1 \text{second}}{14,400 \text{ns}} = 71.11 \text{ MB/s}$.</p> </li> </ul> </li> <li> <p>解答 (16-word block case)</p> <p>For each block, it takes</p> <ol> <li><strong>1 CC</strong> to send the address to memory</li> <li>260ns/(5ns/cycle) = <strong>52 CC</strong> to read memory</li> <li><strong>2 CC</strong> to send the data from the memory <strong>(Overlap Considered)</strong></li> <li><strong>2 CC</strong> needed between each bus operation.</li> </ol> <p>This is a total of <strong>57 CC</strong>.</p> <p>Since there are 256/16 = 16 blocks, the transfer of 256 words takes 57 $\times$ 16 = 912 CC.</p> <p>The latency for the transfer of 256 words is: 912 cycles $\times$ (5ns/cycle) = 4,560 ns.</p> <p>Final answer:</p> <ul> <li> <p>Number of bus transactions per second:</p> <p>$16 \text{ transactions} \times \frac{1 \text{second}}{4,560 \text{ns}} = 3.51 \text{M transactions/s}$.</p> </li> <li> <p>Bandwidth</p> <p>$1024 \text{bytes} \times \frac{1 \text{second}}{4,560 \text{ns}} = 224.56 \text{ MB/s}$.</p> </li> </ul> </li> </ul> <h3 id="interfacing-io-devices-to-the-memory-processor-and-os">Interfacing I/O Devices to the Memory, Processor, and OS</h3> <ul> <li> <p>I/O Characteristics</p> <ol> <li>(shared) 被多个程序共享</li> <li>(interrupts) 使用中断进行交互</li> <li>(complex) 底层控制非常复杂</li> </ol> </li> <li> <p>I/O Communication Types</p> <ol> <li>由 OS 发出指令给到 I/O devices（注意不是硬件）</li> <li>I/O devices 相应指令（无论是否成功完成操作）</li> <li>数据传输，必须发生在 I/O devices 和 memory 之中</li> </ol> </li> <li> <p>How to Give Commands to I/O Devices?</p> <p>【Method 1】Memory-Mapped I/O</p> <p>一种巧妙的设计，即每个 I/O 都与某个内存地址建立映射。当需要访问某个 I/O 时，直接用 <code class="language-plaintext highlighter-rouge">lw</code> 或 <code class="language-plaintext highlighter-rouge">sw</code> 等指令访问对应的 I/O 即可。</p> <p>【Method 2】Special I/O Instructions</p> <p>指令集为 I/O 专门设计的指令。例如 <code class="language-plaintext highlighter-rouge">in al, port</code> 和 <code class="language-plaintext highlighter-rouge">out port, al</code> 等。</p> </li> <li> <p>Communication with Processor</p> <ol> <li> <p><strong>Polling（轮寻）</strong>：定期（如每 10ms）去寻访某个 I/O，看其在过去的 10ms 内有无 I/O 请求。若有则执行。</p> </li> <li> <p><strong>Interrupt（中断）</strong>：当某个 I/O 发出请求，立刻中断 processor 并赶去执行。</p> <p><img src="/assets/img/blog_post/co/cod-118.png" alt="cod-118" width="80%"/></p> <p>从上图可以看出，为了 incept data，printer 会抛出 request interrupt。</p> <p>而每当 printer 抛出一个 request interrupt，都会导致 CPU 产生一个中断（对应 response interrupt），并在这个中断中为 printer 传输数据。</p> </li> <li> <p><strong>Direct Memory Access (DMA)</strong>：</p> <p>实际上是 Interrupt 下的一个子分支，不过加入了 DMA 的优化。简单理解为，CPU 创建了一个专门处理 I/O 事件的「小跟班」DMA。</p> <p>以往 I/O Device 想要和 memory 交互（获取数据），必须要经过 CPU（这是因为 CPU 才能控制 memory）。而这会导致 CPU 被频繁中断，不好。</p> <p>优化后，在某一批次的 I/O 之前，CPU 事先创建并初始化一个 DMA，使其代为执行自己职责。这样只有在这一批 I/O 开始和结尾会导致 CPU 产生中断（共计 2 次，少了很多）。</p> <p><img src="/assets/img/blog_post/co/cod-119.png" alt="cod-119" width="60%"/></p> </li> </ol> </li> <li> <p>Measure the Performance of Polling / Interrupt / DMA</p> <ol> <li> <p>Polling</p> <ul> <li> <p>Question:</p> <p><img src="/assets/img/blog_post/co/cod-120.png" alt="cod-120" width="60%"/></p> </li> <li> <p>Solution:</p> </li> </ul> <p><img src="/assets/img/blog_post/co/cod-121.png" alt="cod-121" width="60%"/></p> </li> <li> <p>Interrupt</p> <ul> <li> <p>Question:</p> <p>Suppose we have the same hard disk and processor we used in the former example, but we used interrupt-driven I/O. The overhead for each transfer, including the interrupt, is 500 clock cycles. Find the fraction of the processor consumed if <strong>the hard disk is only transferring data 5% of the time</strong>.</p> </li> <li> <p>Solution:</p> <p>实际上，最后一句话的假设是使得 Interrupt 在绝大多数情况下效率高于 Polling 的根本原因。考虑一个较长的时间段，比如说 1s。在此期间有 0.05s 我们进行 hard-disk I/O。这 0.05s 内我们需要传输 $0.05 \times 4 \text{MB} = 200 \text{KB}$ 数据。一次 data transfer 为 $16 \text{B}$，所以总共会造成 $12,500$ 次中断，损失 $12,500 \times 500 = 6.25 \times 10^6 \text{CC}$。</p> <p>一秒钟共 $500 \times 10^6 \text{CC}$，所以损失率为 $1.25\%$。</p> </li> </ul> </li> <li> <p>DMA</p> <ul> <li> <p>Question:</p> <p>Suppose we have the same hard disk and processor we used in the former example.</p> <p>Assume that the initial setup of a DMA transfer takes 1000 clock cycles for the processor, and assume the handling of the interrupt at DMA completion requires 500 clock cycles for the processor.</p> <p>The hard disk has a transfer rate of 4MB/sec and uses DMA. The average transfer from disk is 8 KB.<em>（这里的意思是，每次 transfer 的数据量大小 8KB。每次 transfer 之间可能相隔较久，所以需要重新初始化 DMA）</em> Assume the disk is actively transferring 100% of the time.</p> <p>Please find what fraction of the processor time is consumed.</p> </li> <li> <p>Solution:</p> <p>一次 disk transfer 所需时间为 8KB / (4MB / s) = 2ms。</p> <p>由于假设所有时间都在做 disk I/O，所以相当于 1s 内会做 500 批 disk I/O。</p> <p>每批 disk I/O 需要重新建立 DMA，同时打断 CPU，这部分总共消耗 1500 CC。</p> <p>所以 1s 内会使得 CPU 中断 $7.5 \times 10^5 \text{CC}$。损失率为 $0.2\%$。</p> </li> </ul> </li> </ol> </li> </ul>]]></content><author><name></name></author><category term="Course"/><category term="Notes"/><category term="Hardware"/><summary type="html"><![CDATA[Contains content about memory and hierarchy storage.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://sqrtyz.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://sqrtyz.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://sqrtyz.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>